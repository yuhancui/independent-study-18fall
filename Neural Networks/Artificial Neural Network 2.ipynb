{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, I wlil implement a neural network on a dataset with epistatic pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data exploration\n",
    "### Understand raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)# display all the columns\n",
    "raw_data = pd.read_csv('a_20s_1600her_0.4__maf_0.2_EDM-2_01.txt', sep = \"\\t\")# read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N0</th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>N4</th>\n",
       "      <th>N5</th>\n",
       "      <th>N6</th>\n",
       "      <th>N7</th>\n",
       "      <th>N8</th>\n",
       "      <th>N9</th>\n",
       "      <th>N10</th>\n",
       "      <th>N11</th>\n",
       "      <th>N12</th>\n",
       "      <th>N13</th>\n",
       "      <th>N14</th>\n",
       "      <th>N15</th>\n",
       "      <th>N16</th>\n",
       "      <th>N17</th>\n",
       "      <th>M0P0</th>\n",
       "      <th>M0P1</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N0  N1  N2  N3  N4  N5  N6  N7  N8  N9  N10  N11  N12  N13  N14  N15  N16  \\\n",
       "0   0   0   0   0   0   0   0   2   0   0    1    2    1    2    1    0    1   \n",
       "1   1   0   0   0   0   1   1   1   0   0    0    1    0    1    1    0    0   \n",
       "2   0   0   0   1   0   0   0   2   0   0    0    1    0    1    1    0    0   \n",
       "3   0   0   0   1   2   0   1   0   0   0    1    1    1    1    1    1    1   \n",
       "4   0   1   0   1   0   0   0   0   0   0    0    0    1    1    1    0    1   \n",
       "\n",
       "   N17  M0P0  M0P1  Class  \n",
       "0    0     1     1      1  \n",
       "1    1     1     1      1  \n",
       "2    0     0     0      1  \n",
       "3    1     1     1      1  \n",
       "4    0     0     0      1  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1600\n",
      "Number of columns: 21\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of rows: \" + str(raw_data.shape[0])) # row count\n",
    "print (\"Number of columns: \" + str(raw_data.shape[1])) # column count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N0</th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>N4</th>\n",
       "      <th>N5</th>\n",
       "      <th>N6</th>\n",
       "      <th>N7</th>\n",
       "      <th>N8</th>\n",
       "      <th>N9</th>\n",
       "      <th>N10</th>\n",
       "      <th>N11</th>\n",
       "      <th>N12</th>\n",
       "      <th>N13</th>\n",
       "      <th>N14</th>\n",
       "      <th>N15</th>\n",
       "      <th>N16</th>\n",
       "      <th>N17</th>\n",
       "      <th>M0P0</th>\n",
       "      <th>M0P1</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.00000</td>\n",
       "      <td>1600.00000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.00000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.00000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.41250</td>\n",
       "      <td>0.21750</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.888125</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.335625</td>\n",
       "      <td>1.024375</td>\n",
       "      <td>0.114375</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.688750</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.798750</td>\n",
       "      <td>0.85375</td>\n",
       "      <td>0.791875</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.682500</td>\n",
       "      <td>0.33000</td>\n",
       "      <td>0.396875</td>\n",
       "      <td>0.426250</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.439448</td>\n",
       "      <td>0.55343</td>\n",
       "      <td>0.43768</td>\n",
       "      <td>0.687537</td>\n",
       "      <td>0.706874</td>\n",
       "      <td>0.358503</td>\n",
       "      <td>0.523834</td>\n",
       "      <td>0.675697</td>\n",
       "      <td>0.328041</td>\n",
       "      <td>0.652872</td>\n",
       "      <td>0.666822</td>\n",
       "      <td>0.702448</td>\n",
       "      <td>0.701645</td>\n",
       "      <td>0.68202</td>\n",
       "      <td>0.678304</td>\n",
       "      <td>0.481968</td>\n",
       "      <td>0.680612</td>\n",
       "      <td>0.51601</td>\n",
       "      <td>0.561972</td>\n",
       "      <td>0.576427</td>\n",
       "      <td>0.500156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                N0          N1          N2           N3           N4  \\\n",
       "count  1600.000000  1600.00000  1600.00000  1600.000000  1600.000000   \n",
       "mean      0.222500     0.41250     0.21750     0.890625     0.888125   \n",
       "std       0.439448     0.55343     0.43768     0.687537     0.706874   \n",
       "min       0.000000     0.00000     0.00000     0.000000     0.000000   \n",
       "25%       0.000000     0.00000     0.00000     0.000000     0.000000   \n",
       "50%       0.000000     0.00000     0.00000     1.000000     1.000000   \n",
       "75%       0.000000     1.00000     0.00000     1.000000     1.000000   \n",
       "max       2.000000     2.00000     2.00000     2.000000     2.000000   \n",
       "\n",
       "                N5           N6           N7           N8           N9  \\\n",
       "count  1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "mean      0.142500     0.335625     1.024375     0.114375     0.595000   \n",
       "std       0.358503     0.523834     0.675697     0.328041     0.652872   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     1.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     1.000000     0.000000     1.000000   \n",
       "75%       0.000000     1.000000     1.000000     0.000000     1.000000   \n",
       "max       2.000000     2.000000     2.000000     2.000000     2.000000   \n",
       "\n",
       "               N10          N11          N12         N13          N14  \\\n",
       "count  1600.000000  1600.000000  1600.000000  1600.00000  1600.000000   \n",
       "mean      0.688750     0.875000     0.798750     0.85375     0.791875   \n",
       "std       0.666822     0.702448     0.701645     0.68202     0.678304   \n",
       "min       0.000000     0.000000     0.000000     0.00000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.00000     0.000000   \n",
       "50%       1.000000     1.000000     1.000000     1.00000     1.000000   \n",
       "75%       1.000000     1.000000     1.000000     1.00000     1.000000   \n",
       "max       2.000000     2.000000     2.000000     2.00000     2.000000   \n",
       "\n",
       "               N15          N16         N17         M0P0         M0P1  \\\n",
       "count  1600.000000  1600.000000  1600.00000  1600.000000  1600.000000   \n",
       "mean      0.281250     0.682500     0.33000     0.396875     0.426250   \n",
       "std       0.481968     0.680612     0.51601     0.561972     0.576427   \n",
       "min       0.000000     0.000000     0.00000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.00000     0.000000     0.000000   \n",
       "50%       0.000000     1.000000     0.00000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     1.00000     1.000000     1.000000   \n",
       "max       2.000000     2.000000     2.00000     2.000000     2.000000   \n",
       "\n",
       "             Class  \n",
       "count  1600.000000  \n",
       "mean      0.500000  \n",
       "std       0.500156  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.500000  \n",
       "75%       1.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.describe() # descriptive statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.isnull().values.any() # check missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Count')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.distplot(raw_data['Class'],kde=False) # The outcome is labeled as 'class'\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x113789d30>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use heat map to visualize correlation between each column\n",
    "corr = raw_data[['N0','N1','N2','N3','N4','N5','N6','N7','N8','N9','N10','N11','N12','N13','N14','N15','N16','N17','M0P0','M0P1','Class']].corr()\n",
    "sb.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N0</th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>N4</th>\n",
       "      <th>N5</th>\n",
       "      <th>N6</th>\n",
       "      <th>N7</th>\n",
       "      <th>N8</th>\n",
       "      <th>N9</th>\n",
       "      <th>N10</th>\n",
       "      <th>N11</th>\n",
       "      <th>N12</th>\n",
       "      <th>N13</th>\n",
       "      <th>N14</th>\n",
       "      <th>N15</th>\n",
       "      <th>N16</th>\n",
       "      <th>N17</th>\n",
       "      <th>M0P0</th>\n",
       "      <th>M0P1</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009900</td>\n",
       "      <td>0.018111</td>\n",
       "      <td>0.030919</td>\n",
       "      <td>0.005693</td>\n",
       "      <td>0.020920</td>\n",
       "      <td>0.006839</td>\n",
       "      <td>-0.022489</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>-0.006147</td>\n",
       "      <td>0.027328</td>\n",
       "      <td>-0.007091</td>\n",
       "      <td>0.052015</td>\n",
       "      <td>0.048128</td>\n",
       "      <td>0.012782</td>\n",
       "      <td>-0.026944</td>\n",
       "      <td>-0.014574</td>\n",
       "      <td>-0.020629</td>\n",
       "      <td>-0.013390</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>-0.022763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N1</th>\n",
       "      <td>-0.009900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>-0.044069</td>\n",
       "      <td>-0.013049</td>\n",
       "      <td>0.018755</td>\n",
       "      <td>0.039882</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>-0.015458</td>\n",
       "      <td>-0.046214</td>\n",
       "      <td>-0.012837</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.011556</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>-0.016056</td>\n",
       "      <td>0.036048</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>-0.045551</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>-0.015815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N2</th>\n",
       "      <td>0.018111</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>-0.037636</td>\n",
       "      <td>-0.007364</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.008623</td>\n",
       "      <td>-0.025039</td>\n",
       "      <td>-0.005085</td>\n",
       "      <td>-0.010111</td>\n",
       "      <td>-0.019076</td>\n",
       "      <td>-0.037017</td>\n",
       "      <td>-0.011488</td>\n",
       "      <td>0.013625</td>\n",
       "      <td>-0.041093</td>\n",
       "      <td>0.027683</td>\n",
       "      <td>-0.013225</td>\n",
       "      <td>0.025712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N3</th>\n",
       "      <td>0.030919</td>\n",
       "      <td>-0.044069</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.050929</td>\n",
       "      <td>-0.005233</td>\n",
       "      <td>-0.010880</td>\n",
       "      <td>-0.023874</td>\n",
       "      <td>-0.016594</td>\n",
       "      <td>-0.008185</td>\n",
       "      <td>0.029371</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>0.016570</td>\n",
       "      <td>-0.024798</td>\n",
       "      <td>-0.024704</td>\n",
       "      <td>0.017398</td>\n",
       "      <td>-0.042182</td>\n",
       "      <td>-0.000441</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>-0.011687</td>\n",
       "      <td>-0.008184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N4</th>\n",
       "      <td>0.005693</td>\n",
       "      <td>-0.013049</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>-0.050929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006151</td>\n",
       "      <td>-0.018449</td>\n",
       "      <td>0.017497</td>\n",
       "      <td>-0.058058</td>\n",
       "      <td>-0.011512</td>\n",
       "      <td>-0.038097</td>\n",
       "      <td>-0.004251</td>\n",
       "      <td>0.005014</td>\n",
       "      <td>0.019226</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>-0.061781</td>\n",
       "      <td>0.017116</td>\n",
       "      <td>-0.020455</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>0.005063</td>\n",
       "      <td>0.007960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N5</th>\n",
       "      <td>0.020920</td>\n",
       "      <td>0.018755</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>-0.005233</td>\n",
       "      <td>-0.006151</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074854</td>\n",
       "      <td>0.034705</td>\n",
       "      <td>-0.021683</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.015605</td>\n",
       "      <td>-0.008692</td>\n",
       "      <td>0.046952</td>\n",
       "      <td>-0.050273</td>\n",
       "      <td>-0.001408</td>\n",
       "      <td>0.035742</td>\n",
       "      <td>0.024067</td>\n",
       "      <td>-0.017715</td>\n",
       "      <td>-0.007722</td>\n",
       "      <td>-0.006613</td>\n",
       "      <td>-0.027903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N6</th>\n",
       "      <td>0.006839</td>\n",
       "      <td>0.039882</td>\n",
       "      <td>-0.037636</td>\n",
       "      <td>-0.010880</td>\n",
       "      <td>-0.018449</td>\n",
       "      <td>0.074854</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026345</td>\n",
       "      <td>0.031228</td>\n",
       "      <td>-0.017400</td>\n",
       "      <td>-0.005118</td>\n",
       "      <td>0.010410</td>\n",
       "      <td>-0.010088</td>\n",
       "      <td>-0.011315</td>\n",
       "      <td>-0.025058</td>\n",
       "      <td>0.024693</td>\n",
       "      <td>-0.020177</td>\n",
       "      <td>-0.016681</td>\n",
       "      <td>0.033732</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>-0.008354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N7</th>\n",
       "      <td>-0.022489</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>-0.007364</td>\n",
       "      <td>-0.023874</td>\n",
       "      <td>0.017497</td>\n",
       "      <td>0.034705</td>\n",
       "      <td>0.026345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029736</td>\n",
       "      <td>0.040821</td>\n",
       "      <td>-0.023403</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>0.027502</td>\n",
       "      <td>0.024025</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>0.016839</td>\n",
       "      <td>-0.012322</td>\n",
       "      <td>-0.005728</td>\n",
       "      <td>-0.002607</td>\n",
       "      <td>-0.023132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N8</th>\n",
       "      <td>0.001226</td>\n",
       "      <td>-0.015458</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>-0.016594</td>\n",
       "      <td>-0.058058</td>\n",
       "      <td>-0.021683</td>\n",
       "      <td>0.031228</td>\n",
       "      <td>0.029736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026617</td>\n",
       "      <td>-0.034426</td>\n",
       "      <td>0.010517</td>\n",
       "      <td>0.043008</td>\n",
       "      <td>0.046860</td>\n",
       "      <td>-0.022241</td>\n",
       "      <td>0.017924</td>\n",
       "      <td>-0.008116</td>\n",
       "      <td>-0.016219</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>0.017153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N9</th>\n",
       "      <td>-0.006147</td>\n",
       "      <td>-0.046214</td>\n",
       "      <td>0.008623</td>\n",
       "      <td>-0.008185</td>\n",
       "      <td>-0.011512</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>-0.017400</td>\n",
       "      <td>0.040821</td>\n",
       "      <td>0.026617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016247</td>\n",
       "      <td>-0.020455</td>\n",
       "      <td>-0.000560</td>\n",
       "      <td>-0.029172</td>\n",
       "      <td>-0.009695</td>\n",
       "      <td>-0.003478</td>\n",
       "      <td>0.021477</td>\n",
       "      <td>0.049825</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>-0.024578</td>\n",
       "      <td>0.017237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N10</th>\n",
       "      <td>0.027328</td>\n",
       "      <td>-0.012837</td>\n",
       "      <td>-0.025039</td>\n",
       "      <td>0.029371</td>\n",
       "      <td>-0.038097</td>\n",
       "      <td>0.015605</td>\n",
       "      <td>-0.005118</td>\n",
       "      <td>-0.023403</td>\n",
       "      <td>-0.034426</td>\n",
       "      <td>0.016247</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022364</td>\n",
       "      <td>-0.047081</td>\n",
       "      <td>-0.012146</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>-0.019337</td>\n",
       "      <td>-0.042876</td>\n",
       "      <td>-0.050273</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.032984</td>\n",
       "      <td>-0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N11</th>\n",
       "      <td>-0.007091</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.005085</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.004251</td>\n",
       "      <td>-0.008692</td>\n",
       "      <td>0.010410</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>0.010517</td>\n",
       "      <td>-0.020455</td>\n",
       "      <td>0.022364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>-0.014686</td>\n",
       "      <td>-0.086135</td>\n",
       "      <td>-0.043871</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>-0.008911</td>\n",
       "      <td>0.025098</td>\n",
       "      <td>-0.003560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N12</th>\n",
       "      <td>0.052015</td>\n",
       "      <td>-0.011556</td>\n",
       "      <td>-0.010111</td>\n",
       "      <td>0.016570</td>\n",
       "      <td>0.005014</td>\n",
       "      <td>0.046952</td>\n",
       "      <td>-0.010088</td>\n",
       "      <td>0.027502</td>\n",
       "      <td>0.043008</td>\n",
       "      <td>-0.000560</td>\n",
       "      <td>-0.047081</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015562</td>\n",
       "      <td>-0.002649</td>\n",
       "      <td>-0.002658</td>\n",
       "      <td>0.024574</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>-0.054253</td>\n",
       "      <td>0.037501</td>\n",
       "      <td>0.005346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N13</th>\n",
       "      <td>0.048128</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>-0.019076</td>\n",
       "      <td>-0.024798</td>\n",
       "      <td>0.019226</td>\n",
       "      <td>-0.050273</td>\n",
       "      <td>-0.011315</td>\n",
       "      <td>0.024025</td>\n",
       "      <td>0.046860</td>\n",
       "      <td>-0.029172</td>\n",
       "      <td>-0.012146</td>\n",
       "      <td>-0.014686</td>\n",
       "      <td>0.015562</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022578</td>\n",
       "      <td>0.012961</td>\n",
       "      <td>-0.025996</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>0.014471</td>\n",
       "      <td>-0.043361</td>\n",
       "      <td>0.033001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N14</th>\n",
       "      <td>0.012782</td>\n",
       "      <td>-0.016056</td>\n",
       "      <td>-0.037017</td>\n",
       "      <td>-0.024704</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>-0.001408</td>\n",
       "      <td>-0.025058</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>-0.022241</td>\n",
       "      <td>-0.009695</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>-0.086135</td>\n",
       "      <td>-0.002649</td>\n",
       "      <td>-0.022578</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.056134</td>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.035932</td>\n",
       "      <td>-0.007942</td>\n",
       "      <td>0.007903</td>\n",
       "      <td>0.034103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N15</th>\n",
       "      <td>-0.026944</td>\n",
       "      <td>0.036048</td>\n",
       "      <td>-0.011488</td>\n",
       "      <td>0.017398</td>\n",
       "      <td>-0.061781</td>\n",
       "      <td>0.035742</td>\n",
       "      <td>0.024693</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>0.017924</td>\n",
       "      <td>-0.003478</td>\n",
       "      <td>-0.019337</td>\n",
       "      <td>-0.043871</td>\n",
       "      <td>-0.002658</td>\n",
       "      <td>0.012961</td>\n",
       "      <td>-0.056134</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000238</td>\n",
       "      <td>0.008801</td>\n",
       "      <td>0.033264</td>\n",
       "      <td>-0.040097</td>\n",
       "      <td>-0.010377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N16</th>\n",
       "      <td>-0.014574</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>0.013625</td>\n",
       "      <td>-0.042182</td>\n",
       "      <td>0.017116</td>\n",
       "      <td>0.024067</td>\n",
       "      <td>-0.020177</td>\n",
       "      <td>0.016839</td>\n",
       "      <td>-0.008116</td>\n",
       "      <td>0.021477</td>\n",
       "      <td>-0.042876</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.024574</td>\n",
       "      <td>-0.025996</td>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.000238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010043</td>\n",
       "      <td>0.018987</td>\n",
       "      <td>0.019982</td>\n",
       "      <td>0.012860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N17</th>\n",
       "      <td>-0.020629</td>\n",
       "      <td>-0.045551</td>\n",
       "      <td>-0.041093</td>\n",
       "      <td>-0.000441</td>\n",
       "      <td>-0.020455</td>\n",
       "      <td>-0.017715</td>\n",
       "      <td>-0.016681</td>\n",
       "      <td>-0.012322</td>\n",
       "      <td>-0.016219</td>\n",
       "      <td>0.049825</td>\n",
       "      <td>-0.050273</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>-0.035932</td>\n",
       "      <td>0.008801</td>\n",
       "      <td>0.010043</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027066</td>\n",
       "      <td>-0.002229</td>\n",
       "      <td>-0.024232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M0P0</th>\n",
       "      <td>-0.013390</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.027683</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.007722</td>\n",
       "      <td>0.033732</td>\n",
       "      <td>-0.005728</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>-0.008911</td>\n",
       "      <td>-0.054253</td>\n",
       "      <td>0.014471</td>\n",
       "      <td>-0.007942</td>\n",
       "      <td>0.033264</td>\n",
       "      <td>0.018987</td>\n",
       "      <td>-0.027066</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.245826</td>\n",
       "      <td>0.056738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M0P1</th>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>-0.013225</td>\n",
       "      <td>-0.011687</td>\n",
       "      <td>0.005063</td>\n",
       "      <td>-0.006613</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>-0.002607</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>-0.024578</td>\n",
       "      <td>0.032984</td>\n",
       "      <td>0.025098</td>\n",
       "      <td>0.037501</td>\n",
       "      <td>-0.043361</td>\n",
       "      <td>0.007903</td>\n",
       "      <td>-0.040097</td>\n",
       "      <td>0.019982</td>\n",
       "      <td>-0.002229</td>\n",
       "      <td>0.245826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>-0.022763</td>\n",
       "      <td>-0.015815</td>\n",
       "      <td>0.025712</td>\n",
       "      <td>-0.008184</td>\n",
       "      <td>0.007960</td>\n",
       "      <td>-0.027903</td>\n",
       "      <td>-0.008354</td>\n",
       "      <td>-0.023132</td>\n",
       "      <td>0.017153</td>\n",
       "      <td>0.017237</td>\n",
       "      <td>-0.003750</td>\n",
       "      <td>-0.003560</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.034103</td>\n",
       "      <td>-0.010377</td>\n",
       "      <td>0.012860</td>\n",
       "      <td>-0.024232</td>\n",
       "      <td>0.056738</td>\n",
       "      <td>0.010846</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             N0        N1        N2        N3        N4        N5        N6  \\\n",
       "N0     1.000000 -0.009900  0.018111  0.030919  0.005693  0.020920  0.006839   \n",
       "N1    -0.009900  1.000000  0.029562 -0.044069 -0.013049  0.018755  0.039882   \n",
       "N2     0.018111  0.029562  1.000000  0.000130  0.001885  0.001634 -0.037636   \n",
       "N3     0.030919 -0.044069  0.000130  1.000000 -0.050929 -0.005233 -0.010880   \n",
       "N4     0.005693 -0.013049  0.001885 -0.050929  1.000000 -0.006151 -0.018449   \n",
       "N5     0.020920  0.018755  0.001634 -0.005233 -0.006151  1.000000  0.074854   \n",
       "N6     0.006839  0.039882 -0.037636 -0.010880 -0.018449  0.074854  1.000000   \n",
       "N7    -0.022489  0.004871 -0.007364 -0.023874  0.017497  0.034705  0.026345   \n",
       "N8     0.001226 -0.015458  0.005216 -0.016594 -0.058058 -0.021683  0.031228   \n",
       "N9    -0.006147 -0.046214  0.008623 -0.008185 -0.011512  0.000908 -0.017400   \n",
       "N10    0.027328 -0.012837 -0.025039  0.029371 -0.038097  0.015605 -0.005118   \n",
       "N11   -0.007091  0.015283 -0.005085 -0.051635 -0.004251 -0.008692  0.010410   \n",
       "N12    0.052015 -0.011556 -0.010111  0.016570  0.005014  0.046952 -0.010088   \n",
       "N13    0.048128  0.012468 -0.019076 -0.024798  0.019226 -0.050273 -0.011315   \n",
       "N14    0.012782 -0.016056 -0.037017 -0.024704  0.004886 -0.001408 -0.025058   \n",
       "N15   -0.026944  0.036048 -0.011488  0.017398 -0.061781  0.035742  0.024693   \n",
       "N16   -0.014574  0.012535  0.013625 -0.042182  0.017116  0.024067 -0.020177   \n",
       "N17   -0.020629 -0.045551 -0.041093 -0.000441 -0.020455 -0.017715 -0.016681   \n",
       "M0P0  -0.013390  0.000126  0.027683  0.007208  0.004787 -0.007722  0.033732   \n",
       "M0P1   0.000630  0.003284 -0.013225 -0.011687  0.005063 -0.006613  0.008500   \n",
       "Class -0.022763 -0.015815  0.025712 -0.008184  0.007960 -0.027903 -0.008354   \n",
       "\n",
       "             N7        N8        N9       N10       N11       N12       N13  \\\n",
       "N0    -0.022489  0.001226 -0.006147  0.027328 -0.007091  0.052015  0.048128   \n",
       "N1     0.004871 -0.015458 -0.046214 -0.012837  0.015283 -0.011556  0.012468   \n",
       "N2    -0.007364  0.005216  0.008623 -0.025039 -0.005085 -0.010111 -0.019076   \n",
       "N3    -0.023874 -0.016594 -0.008185  0.029371 -0.051635  0.016570 -0.024798   \n",
       "N4     0.017497 -0.058058 -0.011512 -0.038097 -0.004251  0.005014  0.019226   \n",
       "N5     0.034705 -0.021683  0.000908  0.015605 -0.008692  0.046952 -0.050273   \n",
       "N6     0.026345  0.031228 -0.017400 -0.005118  0.010410 -0.010088 -0.011315   \n",
       "N7     1.000000  0.029736  0.040821 -0.023403 -0.000165  0.027502  0.024025   \n",
       "N8     0.029736  1.000000  0.026617 -0.034426  0.010517  0.043008  0.046860   \n",
       "N9     0.040821  0.026617  1.000000  0.016247 -0.020455 -0.000560 -0.029172   \n",
       "N10   -0.023403 -0.034426  0.016247  1.000000  0.022364 -0.047081 -0.012146   \n",
       "N11   -0.000165  0.010517 -0.020455  0.022364  1.000000  0.013640 -0.014686   \n",
       "N12    0.027502  0.043008 -0.000560 -0.047081  0.013640  1.000000  0.015562   \n",
       "N13    0.024025  0.046860 -0.029172 -0.012146 -0.014686  0.015562  1.000000   \n",
       "N14    0.009711 -0.022241 -0.009695  0.036438 -0.086135 -0.002649 -0.022578   \n",
       "N15    0.003901  0.017924 -0.003478 -0.019337 -0.043871 -0.002658  0.012961   \n",
       "N16    0.016839 -0.008116  0.021477 -0.042876  0.008503  0.024574 -0.025996   \n",
       "N17   -0.012322 -0.016219  0.049825 -0.050273  0.005176  0.003904 -0.001386   \n",
       "M0P0  -0.005728  0.014831  0.019048  0.002743 -0.008911 -0.054253  0.014471   \n",
       "M0P1  -0.002607  0.009910 -0.024578  0.032984  0.025098  0.037501 -0.043361   \n",
       "Class -0.023132  0.017153  0.017237 -0.003750 -0.003560  0.005346  0.033001   \n",
       "\n",
       "            N14       N15       N16       N17      M0P0      M0P1     Class  \n",
       "N0     0.012782 -0.026944 -0.014574 -0.020629 -0.013390  0.000630 -0.022763  \n",
       "N1    -0.016056  0.036048  0.012535 -0.045551  0.000126  0.003284 -0.015815  \n",
       "N2    -0.037017 -0.011488  0.013625 -0.041093  0.027683 -0.013225  0.025712  \n",
       "N3    -0.024704  0.017398 -0.042182 -0.000441  0.007208 -0.011687 -0.008184  \n",
       "N4     0.004886 -0.061781  0.017116 -0.020455  0.004787  0.005063  0.007960  \n",
       "N5    -0.001408  0.035742  0.024067 -0.017715 -0.007722 -0.006613 -0.027903  \n",
       "N6    -0.025058  0.024693 -0.020177 -0.016681  0.033732  0.008500 -0.008354  \n",
       "N7     0.009711  0.003901  0.016839 -0.012322 -0.005728 -0.002607 -0.023132  \n",
       "N8    -0.022241  0.017924 -0.008116 -0.016219  0.014831  0.009910  0.017153  \n",
       "N9    -0.009695 -0.003478  0.021477  0.049825  0.019048 -0.024578  0.017237  \n",
       "N10    0.036438 -0.019337 -0.042876 -0.050273  0.002743  0.032984 -0.003750  \n",
       "N11   -0.086135 -0.043871  0.008503  0.005176 -0.008911  0.025098 -0.003560  \n",
       "N12   -0.002649 -0.002658  0.024574  0.003904 -0.054253  0.037501  0.005346  \n",
       "N13   -0.022578  0.012961 -0.025996 -0.001386  0.014471 -0.043361  0.033001  \n",
       "N14    1.000000 -0.056134 -0.033497 -0.035932 -0.007942  0.007903  0.034103  \n",
       "N15   -0.056134  1.000000 -0.000238  0.008801  0.033264 -0.040097 -0.010377  \n",
       "N16   -0.033497 -0.000238  1.000000  0.010043  0.018987  0.019982  0.012860  \n",
       "N17   -0.035932  0.008801  0.010043  1.000000 -0.027066 -0.002229 -0.024232  \n",
       "M0P0  -0.007942  0.033264  0.018987 -0.027066  1.000000  0.245826  0.056738  \n",
       "M0P1   0.007903 -0.040097  0.019982 -0.002229  0.245826  1.000000  0.010846  \n",
       "Class  0.034103 -0.010377  0.012860 -0.024232  0.056738  0.010846  1.000000  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = raw_data.iloc[:, -1].values\n",
    "X = raw_data.iloc[:, :-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Let's build a 2-layer neural network with one input layer, one hidden layer, and one output layer (note that the input layer is typically excluded when counting the number of layers in a Neural Network). \n",
    "\n",
    "The number of nodes in the input layer is equal to the number of features (columns) in my data, 20. \n",
    "The number of nodes in the output layer is determined by the number of classes we have, 2. \n",
    "For the number of nodes in the hidden layer, I will choose 11 which is the mean of the neurons in the input and output layers.\n",
    "\n",
    "We will choose tanh as the activation function for our hidden layer and softmax function for output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural network structure\n",
    "num_input_layer = 20 # number of nodes in input layer\n",
    "num_output_layer = 2 # number of nodes in output layer\n",
    "num_hidden_layer = 11 # number of nodes in hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient descent parameters \n",
    "learning_rate = 0.001\n",
    "regularization_lambda = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function predicts output(0 or 1).It calculate the output of the neural network and return the class with highest probability. \n",
    "def predict(m, x):\n",
    "    w1, w2, b1, b2 = m['w1'],m['w2'],m['b1'],m['b2']\n",
    "    \n",
    "    # feedforward\n",
    "    z1 = x.dot(w1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(w2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This functions calculates total loss\n",
    "def total_loss(m, X = X, y = y):\n",
    "    w1, w2, b1, b2 = m['w1'],m['w2'],m['b1'],m['b2']\n",
    "    train_size = len(X)\n",
    "    # feedforward\n",
    "    z1 = X.dot(w1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(w2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss_sum = np.sum(-np.log(probs[range(train_size), y]))\n",
    "    loss  = 1.0/train_size * loss_sum\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function returns a neural net work model with learned parameters\n",
    "# num_n_hidden: number of nodes in hidden layer\n",
    "# num_iteration: number of passes through the training data for gradient descent\n",
    "# print_loss: print the loss every 1000 iterations when True\n",
    "def nn_model( num_n_hidden, num_iteration,learning_rate, print_loss = False, X = X, y = y):\n",
    "    # we will return a model with parameters at the end: \n",
    "    model = {}\n",
    "    train_size = len(X)\n",
    "    \n",
    "    # initialize parameters \n",
    "    np.random.seed(39)\n",
    "    w1 = np.random.randn(num_input_layer , num_n_hidden) / np.sqrt(num_input_layer )\n",
    "    b1 = np.zeros((1, num_n_hidden))\n",
    "    w2 = np.random.randn(num_n_hidden, num_output_layer) / np.sqrt(num_n_hidden)\n",
    "    b2 = np.zeros((1, num_output_layer))\n",
    "    \n",
    "    # gradient descent\n",
    "    for i in range(0, num_iteration):\n",
    "        # feedforward\n",
    "        z1 = X.dot(w1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(w2) + b2\n",
    "        exp_scores = np.exp(z2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        # backpropagation\n",
    "        delta3 = probs\n",
    "        delta3[range(train_size), y] -= 1\n",
    "        dw2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(w2.T) * (1 - np.power(a1, 2))\n",
    "        dw1 = np.dot(X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dw2 += regularization_lambda * w2\n",
    "        dw1 += regularization_lambda * w1\n",
    "        # Gradient descent parameter update\n",
    "        w1 += -learning_rate * dw1\n",
    "        b1 += -learning_rate * db1\n",
    "        w2 += -learning_rate * dw2\n",
    "        b2 += -learning_rate * db2\n",
    "        \n",
    "        model = {'w1':w1, 'w2':w2, 'b1':b1, 'b2':b2}\n",
    "        \n",
    "        #print loss\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print('Loss after %i iteration:%f' %(i,total_loss(model,X = X, y = y)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function does 10-fold. It saves the result at each time as different parts of y_pred. \n",
    "# In the end, it returns the y_pred as the result of all the 10-fold.\n",
    "def run_cv(X,y,iteration):\n",
    "    # Construct a kfolds object\n",
    "    kf = KFold(len(y),n_folds=10,shuffle=True) # Total number of elements；Number of folds， default=3；Whether to shuffle the data before splitting into batches\n",
    "    y_pred = y.copy()\n",
    "     \n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        \n",
    "        m = nn_model(num_hidden_layer,iteration, learning_rate,print_loss = False,X = X_train, y = y_train)\n",
    "        y_pred[test_index] = predict(m, X_test)\n",
    "         \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a neural network with a hidden layer size of 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration:0.765664\n",
      "Loss after 1000 iteration:0.369510\n",
      "Loss after 2000 iteration:0.272611\n",
      "Loss after 3000 iteration:0.237245\n",
      "Loss after 4000 iteration:0.216944\n",
      "Loss after 5000 iteration:0.205759\n",
      "Loss after 6000 iteration:0.197935\n",
      "Loss after 7000 iteration:0.191417\n",
      "Loss after 8000 iteration:0.186782\n",
      "Loss after 9000 iteration:0.183306\n",
      "Loss after 10000 iteration:0.180020\n",
      "Loss after 11000 iteration:0.177243\n",
      "Loss after 12000 iteration:0.174940\n",
      "Loss after 13000 iteration:0.173447\n",
      "Loss after 14000 iteration:0.173236\n",
      "Loss after 15000 iteration:0.169890\n",
      "Loss after 16000 iteration:0.170789\n",
      "Loss after 17000 iteration:0.166717\n",
      "Loss after 18000 iteration:0.165996\n",
      "Loss after 19000 iteration:0.168925\n"
     ]
    }
   ],
   "source": [
    "# with learning_rate = 0.001\n",
    "model = nn_model(num_hidden_layer,20000, learning_rate,print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = run_cv(X, y,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function calculates accuracy\n",
    "def accuracy(y_true,y_pred):\n",
    "    return np.mean(y_true == y_pred) # NumPy interpretes True and False as 1. and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy: 0.7375\n"
     ]
    }
   ],
   "source": [
    "print('Neural network accuracy: ' + str(accuracy(y, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run more iterations to see if the accuracy gets higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration:0.765664\n",
      "Loss after 1000 iteration:0.369510\n",
      "Loss after 2000 iteration:0.272611\n",
      "Loss after 3000 iteration:0.237245\n",
      "Loss after 4000 iteration:0.216944\n",
      "Loss after 5000 iteration:0.205759\n",
      "Loss after 6000 iteration:0.197935\n",
      "Loss after 7000 iteration:0.191417\n",
      "Loss after 8000 iteration:0.186782\n",
      "Loss after 9000 iteration:0.183306\n",
      "Loss after 10000 iteration:0.180020\n",
      "Loss after 11000 iteration:0.177243\n",
      "Loss after 12000 iteration:0.174940\n",
      "Loss after 13000 iteration:0.173447\n",
      "Loss after 14000 iteration:0.173236\n",
      "Loss after 15000 iteration:0.169890\n",
      "Loss after 16000 iteration:0.170789\n",
      "Loss after 17000 iteration:0.166717\n",
      "Loss after 18000 iteration:0.165996\n",
      "Loss after 19000 iteration:0.168925\n",
      "Loss after 20000 iteration:0.162540\n",
      "Loss after 21000 iteration:0.162035\n",
      "Loss after 22000 iteration:0.160970\n",
      "Loss after 23000 iteration:0.160379\n",
      "Loss after 24000 iteration:0.168081\n",
      "Loss after 25000 iteration:0.164793\n",
      "Loss after 26000 iteration:0.158007\n",
      "Loss after 27000 iteration:0.166841\n",
      "Loss after 28000 iteration:0.158175\n",
      "Loss after 29000 iteration:0.166769\n"
     ]
    }
   ],
   "source": [
    "model_3 = nn_model(num_hidden_layer,30000, learning_rate,print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_3 = run_cv(X, y,30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy: 0.73125\n"
     ]
    }
   ],
   "source": [
    "print('Neural network accuracy: ' + str(accuracy(y, y_pred_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running more iterations did not increase accuracy. one possible reason is that larger iteration numbers lead to overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
