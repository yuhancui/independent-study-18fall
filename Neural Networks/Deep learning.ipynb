{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods.<br>\n",
    "Deep learning architectures such as deep neural networks, deep belief networks and recurrent neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design and board game programs, where they have produced results comparable to and in some cases superior to human experts.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Deep learning is a class of machine learning algorithms that:\n",
    "\n",
    "Use a cascade of multiple layers of nonlinear processing units for feature extraction and transformation. Each successive layer uses the output from the previous layer as input.<br>\n",
    "Learn in supervised (e.g., classification) and/or unsupervised (e.g., pattern analysis) manners.<br>\n",
    "Learn multiple levels of representations that correspond to different levels of abstraction; the levels form a hierarchy of concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data exploration\n",
    "### Understand raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)# display all the columns\n",
    "raw_data = pd.read_csv('20_bit_mutliplexer_2000_01.txt', sep = \"\\t\")# read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_0</th>\n",
       "      <th>A_1</th>\n",
       "      <th>A_2</th>\n",
       "      <th>A_3</th>\n",
       "      <th>R_0</th>\n",
       "      <th>R_1</th>\n",
       "      <th>R_2</th>\n",
       "      <th>R_3</th>\n",
       "      <th>R_4</th>\n",
       "      <th>R_5</th>\n",
       "      <th>R_6</th>\n",
       "      <th>R_7</th>\n",
       "      <th>R_8</th>\n",
       "      <th>R_9</th>\n",
       "      <th>R_10</th>\n",
       "      <th>R_11</th>\n",
       "      <th>R_12</th>\n",
       "      <th>R_13</th>\n",
       "      <th>R_14</th>\n",
       "      <th>R_15</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A_0  A_1  A_2  A_3  R_0  R_1  R_2  R_3  R_4  R_5  R_6  R_7  R_8  R_9  R_10  \\\n",
       "0    1    0    1    1    1    1    1    0    0    1    0    1    1    1     0   \n",
       "1    0    0    0    0    0    1    1    1    0    0    1    0    1    0     0   \n",
       "2    0    0    1    1    1    0    0    0    1    0    0    1    1    1     1   \n",
       "3    0    0    0    0    0    0    1    1    0    1    0    1    0    0     1   \n",
       "4    1    0    1    0    0    0    1    1    0    1    0    0    0    1     1   \n",
       "\n",
       "   R_11  R_12  R_13  R_14  R_15  Class  \n",
       "0     0     0     1     0     0      0  \n",
       "1     0     1     0     1     1      0  \n",
       "2     1     1     0     1     0      0  \n",
       "3     0     1     0     1     0      0  \n",
       "4     1     1     1     1     1      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2000\n",
      "Number of columns: 21\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of rows: \" + str(raw_data.shape[0])) # row count\n",
    "print (\"Number of columns: \" + str(raw_data.shape[1])) # column count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_0</th>\n",
       "      <th>A_1</th>\n",
       "      <th>A_2</th>\n",
       "      <th>A_3</th>\n",
       "      <th>R_0</th>\n",
       "      <th>R_1</th>\n",
       "      <th>R_2</th>\n",
       "      <th>R_3</th>\n",
       "      <th>R_4</th>\n",
       "      <th>R_5</th>\n",
       "      <th>R_6</th>\n",
       "      <th>R_7</th>\n",
       "      <th>R_8</th>\n",
       "      <th>R_9</th>\n",
       "      <th>R_10</th>\n",
       "      <th>R_11</th>\n",
       "      <th>R_12</th>\n",
       "      <th>R_13</th>\n",
       "      <th>R_14</th>\n",
       "      <th>R_15</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.494500</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.489500</td>\n",
       "      <td>0.500500</td>\n",
       "      <td>0.487000</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.487000</td>\n",
       "      <td>0.485500</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.4850</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>0.498500</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500116</td>\n",
       "      <td>0.500025</td>\n",
       "      <td>0.500124</td>\n",
       "      <td>0.500119</td>\n",
       "      <td>0.500095</td>\n",
       "      <td>0.500069</td>\n",
       "      <td>0.500015</td>\n",
       "      <td>0.500125</td>\n",
       "      <td>0.499956</td>\n",
       "      <td>0.500061</td>\n",
       "      <td>0.500025</td>\n",
       "      <td>0.500061</td>\n",
       "      <td>0.499956</td>\n",
       "      <td>0.499915</td>\n",
       "      <td>0.500116</td>\n",
       "      <td>0.4999</td>\n",
       "      <td>0.500076</td>\n",
       "      <td>0.499956</td>\n",
       "      <td>0.500123</td>\n",
       "      <td>0.500124</td>\n",
       "      <td>0.500044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               A_0          A_1          A_2          A_3          R_0  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      0.503000     0.510000     0.499000     0.497500     0.494500   \n",
       "std       0.500116     0.500025     0.500124     0.500119     0.500095   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     1.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "               R_1          R_2          R_3          R_4          R_5  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      0.507500     0.489500     0.500500     0.487000     0.492000   \n",
       "std       0.500069     0.500015     0.500125     0.499956     0.500061   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     1.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "               R_6          R_7          R_8          R_9         R_10  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      0.510000     0.492000     0.487000     0.485500     0.503000   \n",
       "std       0.500025     0.500061     0.499956     0.499915     0.500116   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     1.000000   \n",
       "75%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "            R_11         R_12         R_13         R_14         R_15  \\\n",
       "count  2000.0000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      0.4850     0.507000     0.513000     0.498500     0.499000   \n",
       "std       0.4999     0.500076     0.499956     0.500123     0.500124   \n",
       "min       0.0000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.0000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.0000     1.000000     1.000000     0.000000     0.000000   \n",
       "75%       1.0000     1.000000     1.000000     1.000000     1.000000   \n",
       "max       1.0000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             Class  \n",
       "count  2000.000000  \n",
       "mean      0.491000  \n",
       "std       0.500044  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.describe() # descriptive statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.isnull().values.any() # check missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Count')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.distplot(raw_data['Class'],kde=False) # The outcome is labeled as 'class'\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12031fa58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use heat map to visualize correlation between each column\n",
    "corr = raw_data[['A_0','A_1','A_2','A_3','R_1','R_2','R_3','R_4','R_5','R_6','R_7','R_8','R_9','R_10','R_11','R_12','R_13','R_14','R_15','Class']].corr()\n",
    "sb.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_0</th>\n",
       "      <th>A_1</th>\n",
       "      <th>A_2</th>\n",
       "      <th>A_3</th>\n",
       "      <th>R_1</th>\n",
       "      <th>R_2</th>\n",
       "      <th>R_3</th>\n",
       "      <th>R_4</th>\n",
       "      <th>R_5</th>\n",
       "      <th>R_6</th>\n",
       "      <th>R_7</th>\n",
       "      <th>R_8</th>\n",
       "      <th>R_9</th>\n",
       "      <th>R_10</th>\n",
       "      <th>R_11</th>\n",
       "      <th>R_12</th>\n",
       "      <th>R_13</th>\n",
       "      <th>R_14</th>\n",
       "      <th>R_15</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A_0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011883</td>\n",
       "      <td>0.030013</td>\n",
       "      <td>0.017031</td>\n",
       "      <td>0.016912</td>\n",
       "      <td>0.023132</td>\n",
       "      <td>-0.019006</td>\n",
       "      <td>0.010160</td>\n",
       "      <td>-0.025908</td>\n",
       "      <td>-0.014123</td>\n",
       "      <td>0.044102</td>\n",
       "      <td>0.006158</td>\n",
       "      <td>-0.004828</td>\n",
       "      <td>-0.012036</td>\n",
       "      <td>-0.009825</td>\n",
       "      <td>0.021919</td>\n",
       "      <td>0.017850</td>\n",
       "      <td>-0.000982</td>\n",
       "      <td>-0.013988</td>\n",
       "      <td>0.010110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_1</th>\n",
       "      <td>0.011883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005961</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>-0.015305</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>-0.033027</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>-0.013684</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>0.016325</td>\n",
       "      <td>0.014528</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>-0.004121</td>\n",
       "      <td>-0.013409</td>\n",
       "      <td>-0.004281</td>\n",
       "      <td>-0.012527</td>\n",
       "      <td>0.031066</td>\n",
       "      <td>0.016043</td>\n",
       "      <td>-0.003641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_2</th>\n",
       "      <td>0.030013</td>\n",
       "      <td>-0.005961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028990</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.020963</td>\n",
       "      <td>-0.004998</td>\n",
       "      <td>-0.018058</td>\n",
       "      <td>-0.030036</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>-0.028062</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.038013</td>\n",
       "      <td>0.017948</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.012056</td>\n",
       "      <td>-0.023006</td>\n",
       "      <td>-0.038004</td>\n",
       "      <td>-0.012038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_3</th>\n",
       "      <td>0.017031</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>0.028990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034079</td>\n",
       "      <td>-0.018109</td>\n",
       "      <td>0.024005</td>\n",
       "      <td>0.012875</td>\n",
       "      <td>-0.003080</td>\n",
       "      <td>0.009102</td>\n",
       "      <td>0.028924</td>\n",
       "      <td>0.014875</td>\n",
       "      <td>-0.026156</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>0.050874</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>0.015135</td>\n",
       "      <td>0.053986</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.036916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_1</th>\n",
       "      <td>0.016912</td>\n",
       "      <td>-0.015305</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.034079</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032326</td>\n",
       "      <td>0.017987</td>\n",
       "      <td>-0.012616</td>\n",
       "      <td>-0.034768</td>\n",
       "      <td>0.056718</td>\n",
       "      <td>-0.030767</td>\n",
       "      <td>0.019399</td>\n",
       "      <td>-0.003567</td>\n",
       "      <td>-0.017092</td>\n",
       "      <td>-0.006554</td>\n",
       "      <td>-0.025215</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>0.022048</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.075291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_2</th>\n",
       "      <td>0.023132</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>0.020963</td>\n",
       "      <td>-0.018109</td>\n",
       "      <td>0.032326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014024</td>\n",
       "      <td>-0.001547</td>\n",
       "      <td>0.012668</td>\n",
       "      <td>-0.020589</td>\n",
       "      <td>-0.025345</td>\n",
       "      <td>-0.007550</td>\n",
       "      <td>-0.004612</td>\n",
       "      <td>0.009128</td>\n",
       "      <td>0.016381</td>\n",
       "      <td>0.031304</td>\n",
       "      <td>-0.000454</td>\n",
       "      <td>-0.020068</td>\n",
       "      <td>-0.025048</td>\n",
       "      <td>0.088656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_3</th>\n",
       "      <td>-0.019006</td>\n",
       "      <td>-0.033027</td>\n",
       "      <td>-0.004998</td>\n",
       "      <td>0.024005</td>\n",
       "      <td>0.017987</td>\n",
       "      <td>0.014024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>0.019018</td>\n",
       "      <td>0.018984</td>\n",
       "      <td>-0.008985</td>\n",
       "      <td>-0.024982</td>\n",
       "      <td>-0.003973</td>\n",
       "      <td>-0.009006</td>\n",
       "      <td>-0.028983</td>\n",
       "      <td>-0.013015</td>\n",
       "      <td>0.006976</td>\n",
       "      <td>-0.005997</td>\n",
       "      <td>0.049002</td>\n",
       "      <td>0.087032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_4</th>\n",
       "      <td>0.010160</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>-0.018058</td>\n",
       "      <td>0.012875</td>\n",
       "      <td>-0.012616</td>\n",
       "      <td>-0.001547</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002417</td>\n",
       "      <td>0.044544</td>\n",
       "      <td>-0.004418</td>\n",
       "      <td>0.005328</td>\n",
       "      <td>-0.001755</td>\n",
       "      <td>-0.021852</td>\n",
       "      <td>-0.008787</td>\n",
       "      <td>0.036380</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>-0.047094</td>\n",
       "      <td>0.023956</td>\n",
       "      <td>0.071568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_5</th>\n",
       "      <td>-0.025908</td>\n",
       "      <td>-0.013684</td>\n",
       "      <td>-0.030036</td>\n",
       "      <td>-0.003080</td>\n",
       "      <td>-0.034768</td>\n",
       "      <td>0.012668</td>\n",
       "      <td>0.019018</td>\n",
       "      <td>-0.002417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>0.025751</td>\n",
       "      <td>0.041603</td>\n",
       "      <td>-0.047490</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.025535</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>-0.025596</td>\n",
       "      <td>-0.047054</td>\n",
       "      <td>-0.018034</td>\n",
       "      <td>0.075734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_6</th>\n",
       "      <td>-0.014123</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.009102</td>\n",
       "      <td>0.056718</td>\n",
       "      <td>-0.020589</td>\n",
       "      <td>0.018984</td>\n",
       "      <td>0.044544</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>-0.005483</td>\n",
       "      <td>0.009586</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>-0.021414</td>\n",
       "      <td>-0.014284</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>-0.018944</td>\n",
       "      <td>-0.029966</td>\n",
       "      <td>0.080389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_7</th>\n",
       "      <td>0.044102</td>\n",
       "      <td>0.016325</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.028924</td>\n",
       "      <td>-0.030767</td>\n",
       "      <td>-0.025345</td>\n",
       "      <td>-0.008985</td>\n",
       "      <td>-0.004418</td>\n",
       "      <td>0.025751</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002417</td>\n",
       "      <td>0.046562</td>\n",
       "      <td>0.020099</td>\n",
       "      <td>0.013528</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.020426</td>\n",
       "      <td>0.018955</td>\n",
       "      <td>-0.010033</td>\n",
       "      <td>0.063730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_8</th>\n",
       "      <td>0.006158</td>\n",
       "      <td>0.014528</td>\n",
       "      <td>-0.028062</td>\n",
       "      <td>0.014875</td>\n",
       "      <td>0.019399</td>\n",
       "      <td>-0.007550</td>\n",
       "      <td>-0.024982</td>\n",
       "      <td>0.005328</td>\n",
       "      <td>0.041603</td>\n",
       "      <td>-0.005483</td>\n",
       "      <td>-0.002417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016258</td>\n",
       "      <td>-0.011848</td>\n",
       "      <td>0.007226</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>-0.005080</td>\n",
       "      <td>0.015953</td>\n",
       "      <td>0.047556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_9</th>\n",
       "      <td>-0.004828</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>-0.026156</td>\n",
       "      <td>-0.003567</td>\n",
       "      <td>-0.004612</td>\n",
       "      <td>-0.003973</td>\n",
       "      <td>-0.001755</td>\n",
       "      <td>-0.047490</td>\n",
       "      <td>0.009586</td>\n",
       "      <td>0.046562</td>\n",
       "      <td>0.016258</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>-0.007877</td>\n",
       "      <td>0.011412</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>-0.034101</td>\n",
       "      <td>0.008946</td>\n",
       "      <td>0.042503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_10</th>\n",
       "      <td>-0.012036</td>\n",
       "      <td>-0.004121</td>\n",
       "      <td>0.038013</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>-0.017092</td>\n",
       "      <td>0.009128</td>\n",
       "      <td>-0.009006</td>\n",
       "      <td>-0.021852</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>0.020099</td>\n",
       "      <td>-0.011848</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>-0.010085</td>\n",
       "      <td>0.005846</td>\n",
       "      <td>0.005018</td>\n",
       "      <td>-0.011988</td>\n",
       "      <td>0.040115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_11</th>\n",
       "      <td>-0.009825</td>\n",
       "      <td>-0.013409</td>\n",
       "      <td>0.017948</td>\n",
       "      <td>0.050874</td>\n",
       "      <td>-0.006554</td>\n",
       "      <td>0.016381</td>\n",
       "      <td>-0.028983</td>\n",
       "      <td>-0.008787</td>\n",
       "      <td>0.025535</td>\n",
       "      <td>-0.021414</td>\n",
       "      <td>0.013528</td>\n",
       "      <td>0.007226</td>\n",
       "      <td>-0.007877</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003582</td>\n",
       "      <td>-0.003223</td>\n",
       "      <td>0.024921</td>\n",
       "      <td>0.025952</td>\n",
       "      <td>0.047489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_12</th>\n",
       "      <td>0.021919</td>\n",
       "      <td>-0.004281</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>-0.025215</td>\n",
       "      <td>0.031304</td>\n",
       "      <td>-0.013015</td>\n",
       "      <td>0.036380</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>-0.014284</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>0.011412</td>\n",
       "      <td>-0.010085</td>\n",
       "      <td>-0.003582</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024375</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>-0.007973</td>\n",
       "      <td>0.056267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_13</th>\n",
       "      <td>0.017850</td>\n",
       "      <td>-0.012527</td>\n",
       "      <td>0.012056</td>\n",
       "      <td>0.015135</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>-0.000454</td>\n",
       "      <td>0.006976</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>-0.025596</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>0.020426</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.005846</td>\n",
       "      <td>-0.003223</td>\n",
       "      <td>-0.024375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>-0.011952</td>\n",
       "      <td>0.066501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_14</th>\n",
       "      <td>-0.000982</td>\n",
       "      <td>0.031066</td>\n",
       "      <td>-0.023006</td>\n",
       "      <td>0.053986</td>\n",
       "      <td>0.022048</td>\n",
       "      <td>-0.020068</td>\n",
       "      <td>-0.005997</td>\n",
       "      <td>-0.047094</td>\n",
       "      <td>-0.047054</td>\n",
       "      <td>-0.018944</td>\n",
       "      <td>0.018955</td>\n",
       "      <td>-0.005080</td>\n",
       "      <td>-0.034101</td>\n",
       "      <td>0.005018</td>\n",
       "      <td>0.024921</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.047006</td>\n",
       "      <td>0.068957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_15</th>\n",
       "      <td>-0.013988</td>\n",
       "      <td>0.016043</td>\n",
       "      <td>-0.038004</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>-0.025048</td>\n",
       "      <td>0.049002</td>\n",
       "      <td>0.023956</td>\n",
       "      <td>-0.018034</td>\n",
       "      <td>-0.029966</td>\n",
       "      <td>-0.010033</td>\n",
       "      <td>0.015953</td>\n",
       "      <td>0.008946</td>\n",
       "      <td>-0.011988</td>\n",
       "      <td>0.025952</td>\n",
       "      <td>-0.007973</td>\n",
       "      <td>-0.011952</td>\n",
       "      <td>-0.047006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>0.010110</td>\n",
       "      <td>-0.003641</td>\n",
       "      <td>-0.012038</td>\n",
       "      <td>0.036916</td>\n",
       "      <td>0.075291</td>\n",
       "      <td>0.088656</td>\n",
       "      <td>0.087032</td>\n",
       "      <td>0.071568</td>\n",
       "      <td>0.075734</td>\n",
       "      <td>0.080389</td>\n",
       "      <td>0.063730</td>\n",
       "      <td>0.047556</td>\n",
       "      <td>0.042503</td>\n",
       "      <td>0.040115</td>\n",
       "      <td>0.047489</td>\n",
       "      <td>0.056267</td>\n",
       "      <td>0.066501</td>\n",
       "      <td>0.068957</td>\n",
       "      <td>0.063974</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            A_0       A_1       A_2       A_3       R_1       R_2       R_3  \\\n",
       "A_0    1.000000  0.011883  0.030013  0.017031  0.016912  0.023132 -0.019006   \n",
       "A_1    0.011883  1.000000 -0.005961 -0.000900 -0.015305  0.003421 -0.033027   \n",
       "A_2    0.030013 -0.005961  1.000000  0.028990  0.015032  0.020963 -0.004998   \n",
       "A_3    0.017031 -0.000900  0.028990  1.000000  0.034079 -0.018109  0.024005   \n",
       "R_1    0.016912 -0.015305  0.015032  0.034079  1.000000  0.032326  0.017987   \n",
       "R_2    0.023132  0.003421  0.020963 -0.018109  0.032326  1.000000  0.014024   \n",
       "R_3   -0.019006 -0.033027 -0.004998  0.024005  0.017987  0.014024  1.000000   \n",
       "R_4    0.010160  0.000520 -0.018058  0.012875 -0.012616 -0.001547  0.007028   \n",
       "R_5   -0.025908 -0.013684 -0.030036 -0.003080 -0.034768  0.012668  0.019018   \n",
       "R_6   -0.014123  0.023609  0.000040  0.009102  0.056718 -0.020589  0.018984   \n",
       "R_7    0.044102  0.016325  0.003969  0.028924 -0.030767 -0.025345 -0.008985   \n",
       "R_8    0.006158  0.014528 -0.028062  0.014875  0.019399 -0.007550 -0.024982   \n",
       "R_9   -0.004828  0.005583  0.014948 -0.026156 -0.003567 -0.004612 -0.003973   \n",
       "R_10  -0.012036 -0.004121  0.038013  0.007030 -0.017092  0.009128 -0.009006   \n",
       "R_11  -0.009825 -0.013409  0.017948  0.050874 -0.006554  0.016381 -0.028983   \n",
       "R_12   0.021919 -0.004281  0.000028  0.021072 -0.025215  0.031304 -0.013015   \n",
       "R_13   0.017850 -0.012527  0.012056  0.015135  0.002611 -0.000454  0.006976   \n",
       "R_14  -0.000982  0.031066 -0.023006  0.053986  0.022048 -0.020068 -0.005997   \n",
       "R_15  -0.013988  0.016043 -0.038004  0.008990  0.015032 -0.025048  0.049002   \n",
       "Class  0.010110 -0.003641 -0.012038  0.036916  0.075291  0.088656  0.087032   \n",
       "\n",
       "            R_4       R_5       R_6       R_7       R_8       R_9      R_10  \\\n",
       "A_0    0.010160 -0.025908 -0.014123  0.044102  0.006158 -0.004828 -0.012036   \n",
       "A_1    0.000520 -0.013684  0.023609  0.016325  0.014528  0.005583 -0.004121   \n",
       "A_2   -0.018058 -0.030036  0.000040  0.003969 -0.028062  0.014948  0.038013   \n",
       "A_3    0.012875 -0.003080  0.009102  0.028924  0.014875 -0.026156  0.007030   \n",
       "R_1   -0.012616 -0.034768  0.056718 -0.030767  0.019399 -0.003567 -0.017092   \n",
       "R_2   -0.001547  0.012668 -0.020589 -0.025345 -0.007550 -0.004612  0.009128   \n",
       "R_3    0.007028  0.019018  0.018984 -0.008985 -0.024982 -0.003973 -0.009006   \n",
       "R_4    1.000000 -0.002417  0.044544 -0.004418  0.005328 -0.001755 -0.021852   \n",
       "R_5   -0.002417  1.000000  0.020327  0.025751  0.041603 -0.047490  0.008097   \n",
       "R_6    0.044544  0.020327  1.000000  0.008323 -0.005483  0.009586  0.027886   \n",
       "R_7   -0.004418  0.025751  0.008323  1.000000 -0.002417  0.046562  0.020099   \n",
       "R_8    0.005328  0.041603 -0.005483 -0.002417  1.000000  0.016258 -0.011848   \n",
       "R_9   -0.001755 -0.047490  0.009586  0.046562  0.016258  1.000000  0.003175   \n",
       "R_10  -0.021852  0.008097  0.027886  0.020099 -0.011848  0.003175  1.000000   \n",
       "R_11  -0.008787  0.025535 -0.021414  0.013528  0.007226 -0.007877  0.004182   \n",
       "R_12   0.036380  0.000224 -0.014284  0.002225  0.016371  0.011412 -0.010085   \n",
       "R_13   0.014686 -0.025596  0.007484  0.020426  0.014686  0.003757  0.005846   \n",
       "R_14  -0.047094 -0.047054 -0.018944  0.018955 -0.005080 -0.034101  0.005018   \n",
       "R_15   0.023956 -0.018034 -0.029966 -0.010033  0.015953  0.008946 -0.011988   \n",
       "Class  0.071568  0.075734  0.080389  0.063730  0.047556  0.042503  0.040115   \n",
       "\n",
       "           R_11      R_12      R_13      R_14      R_15     Class  \n",
       "A_0   -0.009825  0.021919  0.017850 -0.000982 -0.013988  0.010110  \n",
       "A_1   -0.013409 -0.004281 -0.012527  0.031066  0.016043 -0.003641  \n",
       "A_2    0.017948  0.000028  0.012056 -0.023006 -0.038004 -0.012038  \n",
       "A_3    0.050874  0.021072  0.015135  0.053986  0.008990  0.036916  \n",
       "R_1   -0.006554 -0.025215  0.002611  0.022048  0.015032  0.075291  \n",
       "R_2    0.016381  0.031304 -0.000454 -0.020068 -0.025048  0.088656  \n",
       "R_3   -0.028983 -0.013015  0.006976 -0.005997  0.049002  0.087032  \n",
       "R_4   -0.008787  0.036380  0.014686 -0.047094  0.023956  0.071568  \n",
       "R_5    0.025535  0.000224 -0.025596 -0.047054 -0.018034  0.075734  \n",
       "R_6   -0.021414 -0.014284  0.007484 -0.018944 -0.029966  0.080389  \n",
       "R_7    0.013528  0.002225  0.020426  0.018955 -0.010033  0.063730  \n",
       "R_8    0.007226  0.016371  0.014686 -0.005080  0.015953  0.047556  \n",
       "R_9   -0.007877  0.011412  0.003757 -0.034101  0.008946  0.042503  \n",
       "R_10   0.004182 -0.010085  0.005846  0.005018 -0.011988  0.040115  \n",
       "R_11   1.000000 -0.003582 -0.003223  0.024921  0.025952  0.047489  \n",
       "R_12  -0.003582  1.000000 -0.024375  0.001042 -0.007973  0.056267  \n",
       "R_13  -0.003223 -0.024375  1.000000  0.009081 -0.011952  0.066501  \n",
       "R_14   0.024921  0.001042  0.009081  1.000000 -0.047006  0.068957  \n",
       "R_15   0.025952 -0.007973 -0.011952 -0.047006  1.000000  0.063974  \n",
       "Class  0.047489  0.056267  0.066501  0.068957  0.063974  1.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = raw_data.iloc[:, -1].values\n",
    "X = raw_data.iloc[:, :-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.23% +/- 3.27%\n"
     ]
    }
   ],
   "source": [
    "for train, test in kfold.split(X, y):\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Dense(11, input_dim=20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50,batch_size=10, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "print(\"Accuracy: \" + \"%.2f%% +/- %.2f%%\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Change activation function  \n",
    "Try TanH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.29% +/- 2.37%\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Dense(11, input_dim=20, activation='tanh'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50,batch_size=10, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "print(\"Accuracy: \" + \"%.2f%% +/- %.2f%%\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.85% +/- 4.59%\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Dense(11, input_dim=20, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50,batch_size=10, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "print(\"Accuracy: \" + \"%.2f%% +/- %.2f%%\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, activation function relu seems to have the best performance. Thus, I will continue with relu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change hidden layer size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a smaller hidden layer size: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.65% +/- 4.37%\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, input_dim=20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50,batch_size=10, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "print(\"Accuracy: \" + \"%.2f%% +/- %.2f%%\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a bigger hidden layer size: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.35% +/- 2.31%\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50,batch_size=10, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "print(\"Accuracy: \" + \"%.2f%% +/- %.2f%%\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer size 16 has better performance here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change hidden layer number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try two hidden layers with size 16 and 6, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.30% +/- 3.20%\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=20, activation='relu'))\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50,batch_size=10, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "print(\"Accuracy: \" + \"%.2f%% +/- %.2f%%\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try two hidden layers with size 16 and 11, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.80% +/- 2.56%\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=20, activation='relu'))\n",
    "    model.add(Dense(11, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50,batch_size=10, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "print(\"Accuracy: \" + \"%.2f%% +/- %.2f%%\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try two hidden layers with size 16 and 16, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.05% +/- 3.57%\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=20, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50,batch_size=10, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "print(\"Accuracy: \" + \"%.2f%% +/- %.2f%%\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try two hidden layers with size 16 and 21, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.40% +/- 3.70%\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=20, activation='relu'))\n",
    "    model.add(Dense(21, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50,batch_size=10, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "print(\"Accuracy: \" + \"%.2f%% +/- %.2f%%\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try two hidden layers with size 16 and 26, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.80% +/- 3.11%\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=20, activation='relu'))\n",
    "    model.add(Dense(26, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50,batch_size=10, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "print(\"Accuracy: \" + \"%.2f%% +/- %.2f%%\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try two hidden layers with size 16 and 31, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.05% +/- 2.72%\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=20, activation='relu'))\n",
    "    model.add(Dense(31, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50,batch_size=10, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "print(\"Accuracy: \" + \"%.2f%% +/- %.2f%%\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try two hidden layers with size 16 and 41, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.95% +/- 3.17%\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=20, activation='relu'))\n",
    "    model.add(Dense(41, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50,batch_size=10, verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "print(\"Accuracy: \" + \"%.2f%% +/- %.2f%%\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:<br>\n",
    "https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/<br>\n",
    "https://keras.io/models/model/<br>\n",
    "https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
