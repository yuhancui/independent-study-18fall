{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another.\n",
    "Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains.The neural network itself isn't an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs.Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.\n",
    "\n",
    "In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called 'edges'. Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.\n",
    "\n",
    "The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![title](nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data exploration\n",
    "### Understand raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)# display all the columns\n",
    "raw_data = pd.read_csv('a_20s_1600_Het_h_0.4MAF_0.2_r_50_EDM-2_01.txt', sep = \"\\t\")# read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N0</th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>N4</th>\n",
       "      <th>N5</th>\n",
       "      <th>N6</th>\n",
       "      <th>N7</th>\n",
       "      <th>N8</th>\n",
       "      <th>N9</th>\n",
       "      <th>N10</th>\n",
       "      <th>N11</th>\n",
       "      <th>N12</th>\n",
       "      <th>N13</th>\n",
       "      <th>N14</th>\n",
       "      <th>N15</th>\n",
       "      <th>M0P0</th>\n",
       "      <th>M1P0</th>\n",
       "      <th>M2P0</th>\n",
       "      <th>M3P0</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N0  N1  N2  N3  N4  N5  N6  N7  N8  N9  N10  N11  N12  N13  N14  N15  M0P0  \\\n",
       "0   0   0   1   1   0   0   2   0   0   2    2    0    0    1    2    0     1   \n",
       "1   0   2   1   1   0   1   0   0   0   1    1    0    0    2    1    0     1   \n",
       "2   0   0   0   0   0   1   0   0   0   1    0    0    0    1    1    0     1   \n",
       "3   0   2   0   1   0   0   0   0   0   1    1    0    0    2    0    0     1   \n",
       "4   0   0   1   1   0   0   1   1   0   0    0    0    0    1    1    1     1   \n",
       "\n",
       "   M1P0  M2P0  M3P0  Class  \n",
       "0     1     1     1      1  \n",
       "1     1     1     1      1  \n",
       "2     1     0     1      1  \n",
       "3     1     1     1      1  \n",
       "4     1     1     0      1  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1600\n",
      "Number of columns: 21\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of rows: \" + str(raw_data.shape[0])) # row count\n",
    "print (\"Number of columns: \" + str(raw_data.shape[1])) # column count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N0</th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>N4</th>\n",
       "      <th>N5</th>\n",
       "      <th>N6</th>\n",
       "      <th>N7</th>\n",
       "      <th>N8</th>\n",
       "      <th>N9</th>\n",
       "      <th>N10</th>\n",
       "      <th>N11</th>\n",
       "      <th>N12</th>\n",
       "      <th>N13</th>\n",
       "      <th>N14</th>\n",
       "      <th>N15</th>\n",
       "      <th>M0P0</th>\n",
       "      <th>M1P0</th>\n",
       "      <th>M2P0</th>\n",
       "      <th>M3P0</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.00000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.00000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.081250</td>\n",
       "      <td>0.719375</td>\n",
       "      <td>0.379375</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.488125</td>\n",
       "      <td>0.653125</td>\n",
       "      <td>0.354375</td>\n",
       "      <td>1.007500</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.080625</td>\n",
       "      <td>0.258750</td>\n",
       "      <td>0.83875</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.403750</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.53000</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.516250</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.275583</td>\n",
       "      <td>0.675211</td>\n",
       "      <td>0.560710</td>\n",
       "      <td>0.691202</td>\n",
       "      <td>0.264173</td>\n",
       "      <td>0.703671</td>\n",
       "      <td>0.611427</td>\n",
       "      <td>0.668455</td>\n",
       "      <td>0.541042</td>\n",
       "      <td>0.715202</td>\n",
       "      <td>0.630495</td>\n",
       "      <td>0.285790</td>\n",
       "      <td>0.477693</td>\n",
       "      <td>0.71191</td>\n",
       "      <td>0.687601</td>\n",
       "      <td>0.569815</td>\n",
       "      <td>0.579275</td>\n",
       "      <td>0.60152</td>\n",
       "      <td>0.581902</td>\n",
       "      <td>0.572224</td>\n",
       "      <td>0.500156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                N0           N1           N2           N3           N4  \\\n",
       "count  1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "mean      0.081250     0.719375     0.379375     0.806250     0.072500   \n",
       "std       0.275583     0.675211     0.560710     0.691202     0.264173   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     1.000000     0.000000     1.000000     0.000000   \n",
       "75%       0.000000     1.000000     1.000000     1.000000     0.000000   \n",
       "max       2.000000     2.000000     2.000000     2.000000     2.000000   \n",
       "\n",
       "                N5           N6           N7           N8           N9  \\\n",
       "count  1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "mean      0.837500     0.488125     0.653125     0.354375     1.007500   \n",
       "std       0.703671     0.611427     0.668455     0.541042     0.715202   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     1.000000     0.000000     1.000000   \n",
       "75%       1.000000     1.000000     1.000000     1.000000     2.000000   \n",
       "max       2.000000     2.000000     2.000000     2.000000     2.000000   \n",
       "\n",
       "               N10          N11          N12         N13          N14  \\\n",
       "count  1600.000000  1600.000000  1600.000000  1600.00000  1600.000000   \n",
       "mean      0.515000     0.080625     0.258750     0.83875     0.900000   \n",
       "std       0.630495     0.285790     0.477693     0.71191     0.687601   \n",
       "min       0.000000     0.000000     0.000000     0.00000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.00000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     1.00000     1.000000   \n",
       "75%       1.000000     0.000000     0.000000     1.00000     1.000000   \n",
       "max       2.000000     2.000000     2.000000     2.00000     2.000000   \n",
       "\n",
       "               N15         M0P0        M1P0         M2P0         M3P0  \\\n",
       "count  1600.000000  1600.000000  1600.00000  1600.000000  1600.000000   \n",
       "mean      0.403750     0.530000     0.53000     0.518750     0.516250   \n",
       "std       0.569815     0.579275     0.60152     0.581902     0.572224   \n",
       "min       0.000000     0.000000     0.00000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.00000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.00000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     1.00000     1.000000     1.000000   \n",
       "max       2.000000     2.000000     2.00000     2.000000     2.000000   \n",
       "\n",
       "             Class  \n",
       "count  1600.000000  \n",
       "mean      0.500000  \n",
       "std       0.500156  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.500000  \n",
       "75%       1.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.describe() # descriptive statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.isnull().values.any() # check missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Understand the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Count')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.distplot(raw_data['Class'],kde=False) # The outcome is labeled as 'class'\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x113c82470>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use heat map to visualize correlation between each column\n",
    "corr = raw_data[['N0','N1','N2','N3','N4','N5','N6','N7','N8','N9','N10','N11','N12','N13','N14','N15','M0P0','M1P0','M2P0','M3P0','Class']].corr()\n",
    "sb.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From heat map, we noticed that N0P0, M1P0, M2P0 and M3P0 are correlated with 'Class'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N0</th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>N4</th>\n",
       "      <th>N5</th>\n",
       "      <th>N6</th>\n",
       "      <th>N7</th>\n",
       "      <th>N8</th>\n",
       "      <th>N9</th>\n",
       "      <th>N10</th>\n",
       "      <th>N11</th>\n",
       "      <th>N12</th>\n",
       "      <th>N13</th>\n",
       "      <th>N14</th>\n",
       "      <th>N15</th>\n",
       "      <th>M0P0</th>\n",
       "      <th>M1P0</th>\n",
       "      <th>M2P0</th>\n",
       "      <th>M3P0</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028505</td>\n",
       "      <td>0.027041</td>\n",
       "      <td>0.017031</td>\n",
       "      <td>-0.012241</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.010503</td>\n",
       "      <td>0.016489</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>0.054169</td>\n",
       "      <td>-0.027643</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>0.006256</td>\n",
       "      <td>0.036304</td>\n",
       "      <td>-0.009907</td>\n",
       "      <td>0.008227</td>\n",
       "      <td>0.019241</td>\n",
       "      <td>0.006094</td>\n",
       "      <td>0.015417</td>\n",
       "      <td>0.004537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N1</th>\n",
       "      <td>0.028505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010472</td>\n",
       "      <td>0.045569</td>\n",
       "      <td>0.008949</td>\n",
       "      <td>-0.009164</td>\n",
       "      <td>0.044185</td>\n",
       "      <td>-0.043989</td>\n",
       "      <td>-0.022059</td>\n",
       "      <td>0.017311</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>-0.022036</td>\n",
       "      <td>-0.026798</td>\n",
       "      <td>-0.042155</td>\n",
       "      <td>0.009564</td>\n",
       "      <td>-0.032048</td>\n",
       "      <td>-0.014438</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.007829</td>\n",
       "      <td>0.004526</td>\n",
       "      <td>-0.004630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N2</th>\n",
       "      <td>0.027041</td>\n",
       "      <td>0.010472</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023226</td>\n",
       "      <td>0.046411</td>\n",
       "      <td>0.039052</td>\n",
       "      <td>0.014061</td>\n",
       "      <td>-0.019100</td>\n",
       "      <td>0.051320</td>\n",
       "      <td>-0.035171</td>\n",
       "      <td>0.036079</td>\n",
       "      <td>-0.007569</td>\n",
       "      <td>-0.014152</td>\n",
       "      <td>0.020177</td>\n",
       "      <td>-0.026440</td>\n",
       "      <td>0.017467</td>\n",
       "      <td>-0.014845</td>\n",
       "      <td>-0.014296</td>\n",
       "      <td>-0.007439</td>\n",
       "      <td>-0.041641</td>\n",
       "      <td>-0.005575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N3</th>\n",
       "      <td>0.017031</td>\n",
       "      <td>0.045569</td>\n",
       "      <td>-0.023226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022348</td>\n",
       "      <td>0.005947</td>\n",
       "      <td>-0.005448</td>\n",
       "      <td>0.042595</td>\n",
       "      <td>0.024844</td>\n",
       "      <td>-0.062843</td>\n",
       "      <td>0.025329</td>\n",
       "      <td>0.009478</td>\n",
       "      <td>-0.028009</td>\n",
       "      <td>0.047040</td>\n",
       "      <td>-0.010527</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>-0.012027</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>-0.004956</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.018090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N4</th>\n",
       "      <td>-0.012241</td>\n",
       "      <td>0.008949</td>\n",
       "      <td>0.046411</td>\n",
       "      <td>-0.022348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017326</td>\n",
       "      <td>-0.014026</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.030221</td>\n",
       "      <td>-0.040326</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>0.014793</td>\n",
       "      <td>-0.010957</td>\n",
       "      <td>0.029609</td>\n",
       "      <td>0.038077</td>\n",
       "      <td>-0.034656</td>\n",
       "      <td>-0.033374</td>\n",
       "      <td>-0.033258</td>\n",
       "      <td>-0.061581</td>\n",
       "      <td>-0.056799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N5</th>\n",
       "      <td>0.000403</td>\n",
       "      <td>-0.009164</td>\n",
       "      <td>0.039052</td>\n",
       "      <td>0.005947</td>\n",
       "      <td>-0.017326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011501</td>\n",
       "      <td>0.070218</td>\n",
       "      <td>0.019938</td>\n",
       "      <td>0.022306</td>\n",
       "      <td>0.023822</td>\n",
       "      <td>0.030982</td>\n",
       "      <td>-0.008791</td>\n",
       "      <td>-0.016136</td>\n",
       "      <td>0.014218</td>\n",
       "      <td>-0.010957</td>\n",
       "      <td>0.005830</td>\n",
       "      <td>0.032210</td>\n",
       "      <td>0.030356</td>\n",
       "      <td>0.039178</td>\n",
       "      <td>0.031985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N6</th>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.044185</td>\n",
       "      <td>0.014061</td>\n",
       "      <td>-0.005448</td>\n",
       "      <td>-0.014026</td>\n",
       "      <td>0.011501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009042</td>\n",
       "      <td>-0.020355</td>\n",
       "      <td>-0.024109</td>\n",
       "      <td>-0.006838</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.031939</td>\n",
       "      <td>0.014276</td>\n",
       "      <td>-0.062328</td>\n",
       "      <td>-0.005975</td>\n",
       "      <td>-0.005174</td>\n",
       "      <td>-0.004982</td>\n",
       "      <td>-0.012557</td>\n",
       "      <td>-0.028942</td>\n",
       "      <td>-0.015338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N7</th>\n",
       "      <td>0.010503</td>\n",
       "      <td>-0.043989</td>\n",
       "      <td>-0.019100</td>\n",
       "      <td>0.042595</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.070218</td>\n",
       "      <td>0.009042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012661</td>\n",
       "      <td>0.005445</td>\n",
       "      <td>-0.004711</td>\n",
       "      <td>0.012266</td>\n",
       "      <td>0.012939</td>\n",
       "      <td>-0.040074</td>\n",
       "      <td>-0.007484</td>\n",
       "      <td>0.037897</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>-0.033914</td>\n",
       "      <td>-0.031852</td>\n",
       "      <td>-0.010288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N8</th>\n",
       "      <td>0.016489</td>\n",
       "      <td>-0.022059</td>\n",
       "      <td>0.051320</td>\n",
       "      <td>0.024844</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.019938</td>\n",
       "      <td>-0.020355</td>\n",
       "      <td>-0.012661</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006873</td>\n",
       "      <td>0.003657</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>-0.011400</td>\n",
       "      <td>0.007191</td>\n",
       "      <td>-0.039169</td>\n",
       "      <td>-0.009993</td>\n",
       "      <td>-0.007004</td>\n",
       "      <td>-0.016353</td>\n",
       "      <td>0.025563</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.001156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N9</th>\n",
       "      <td>0.009598</td>\n",
       "      <td>0.017311</td>\n",
       "      <td>-0.035171</td>\n",
       "      <td>-0.062843</td>\n",
       "      <td>0.030221</td>\n",
       "      <td>0.022306</td>\n",
       "      <td>-0.024109</td>\n",
       "      <td>0.005445</td>\n",
       "      <td>-0.006873</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024714</td>\n",
       "      <td>-0.018259</td>\n",
       "      <td>-0.014836</td>\n",
       "      <td>-0.030787</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>0.006376</td>\n",
       "      <td>-0.018658</td>\n",
       "      <td>0.032912</td>\n",
       "      <td>0.049251</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.033218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N10</th>\n",
       "      <td>0.054169</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.036079</td>\n",
       "      <td>0.025329</td>\n",
       "      <td>-0.040326</td>\n",
       "      <td>0.023822</td>\n",
       "      <td>-0.006838</td>\n",
       "      <td>-0.004711</td>\n",
       "      <td>0.003657</td>\n",
       "      <td>0.024714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015393</td>\n",
       "      <td>-0.017048</td>\n",
       "      <td>-0.014114</td>\n",
       "      <td>0.013560</td>\n",
       "      <td>-0.056905</td>\n",
       "      <td>-0.008082</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>-0.017813</td>\n",
       "      <td>-0.019744</td>\n",
       "      <td>-0.019832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N11</th>\n",
       "      <td>-0.027643</td>\n",
       "      <td>-0.022036</td>\n",
       "      <td>-0.007569</td>\n",
       "      <td>0.009478</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>0.030982</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.012266</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>-0.018259</td>\n",
       "      <td>-0.015393</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010897</td>\n",
       "      <td>-0.074383</td>\n",
       "      <td>0.047419</td>\n",
       "      <td>0.030401</td>\n",
       "      <td>-0.050507</td>\n",
       "      <td>-0.008622</td>\n",
       "      <td>-0.048582</td>\n",
       "      <td>-0.009929</td>\n",
       "      <td>-0.037189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N12</th>\n",
       "      <td>0.020725</td>\n",
       "      <td>-0.026798</td>\n",
       "      <td>-0.014152</td>\n",
       "      <td>-0.028009</td>\n",
       "      <td>0.014793</td>\n",
       "      <td>-0.008791</td>\n",
       "      <td>0.031939</td>\n",
       "      <td>0.012939</td>\n",
       "      <td>-0.011400</td>\n",
       "      <td>-0.014836</td>\n",
       "      <td>-0.017048</td>\n",
       "      <td>-0.010897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.020675</td>\n",
       "      <td>-0.052550</td>\n",
       "      <td>0.011137</td>\n",
       "      <td>-0.043890</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>0.007284</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.002618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N13</th>\n",
       "      <td>0.006256</td>\n",
       "      <td>-0.042155</td>\n",
       "      <td>0.020177</td>\n",
       "      <td>0.047040</td>\n",
       "      <td>-0.010957</td>\n",
       "      <td>-0.016136</td>\n",
       "      <td>0.014276</td>\n",
       "      <td>-0.040074</td>\n",
       "      <td>0.007191</td>\n",
       "      <td>-0.030787</td>\n",
       "      <td>-0.014114</td>\n",
       "      <td>-0.074383</td>\n",
       "      <td>-0.020675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018908</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.022353</td>\n",
       "      <td>-0.012063</td>\n",
       "      <td>-0.007794</td>\n",
       "      <td>-0.011986</td>\n",
       "      <td>0.008782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N14</th>\n",
       "      <td>0.036304</td>\n",
       "      <td>0.009564</td>\n",
       "      <td>-0.026440</td>\n",
       "      <td>-0.010527</td>\n",
       "      <td>0.029609</td>\n",
       "      <td>0.014218</td>\n",
       "      <td>-0.062328</td>\n",
       "      <td>-0.007484</td>\n",
       "      <td>-0.039169</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>0.013560</td>\n",
       "      <td>0.047419</td>\n",
       "      <td>-0.052550</td>\n",
       "      <td>-0.018908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015323</td>\n",
       "      <td>-0.014445</td>\n",
       "      <td>0.051107</td>\n",
       "      <td>-0.012504</td>\n",
       "      <td>0.010490</td>\n",
       "      <td>-0.010911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N15</th>\n",
       "      <td>-0.009907</td>\n",
       "      <td>-0.032048</td>\n",
       "      <td>0.017467</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.038077</td>\n",
       "      <td>-0.010957</td>\n",
       "      <td>-0.005975</td>\n",
       "      <td>0.037897</td>\n",
       "      <td>-0.009993</td>\n",
       "      <td>0.006376</td>\n",
       "      <td>-0.056905</td>\n",
       "      <td>0.030401</td>\n",
       "      <td>0.011137</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.015323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019667</td>\n",
       "      <td>-0.004343</td>\n",
       "      <td>-0.019073</td>\n",
       "      <td>-0.014380</td>\n",
       "      <td>-0.030721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M0P0</th>\n",
       "      <td>0.008227</td>\n",
       "      <td>-0.014438</td>\n",
       "      <td>-0.014845</td>\n",
       "      <td>-0.012027</td>\n",
       "      <td>-0.034656</td>\n",
       "      <td>0.005830</td>\n",
       "      <td>-0.005174</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>-0.007004</td>\n",
       "      <td>-0.018658</td>\n",
       "      <td>-0.008082</td>\n",
       "      <td>-0.050507</td>\n",
       "      <td>-0.043890</td>\n",
       "      <td>0.022353</td>\n",
       "      <td>-0.014445</td>\n",
       "      <td>-0.019667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.334838</td>\n",
       "      <td>0.378669</td>\n",
       "      <td>0.334359</td>\n",
       "      <td>0.595758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M1P0</th>\n",
       "      <td>0.019241</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>-0.014296</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>-0.033374</td>\n",
       "      <td>0.032210</td>\n",
       "      <td>-0.004982</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>-0.016353</td>\n",
       "      <td>0.032912</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>-0.008622</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>-0.012063</td>\n",
       "      <td>0.051107</td>\n",
       "      <td>-0.004343</td>\n",
       "      <td>0.334838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377172</td>\n",
       "      <td>0.349248</td>\n",
       "      <td>0.594514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M2P0</th>\n",
       "      <td>0.006094</td>\n",
       "      <td>0.007829</td>\n",
       "      <td>-0.007439</td>\n",
       "      <td>-0.004956</td>\n",
       "      <td>-0.033258</td>\n",
       "      <td>0.030356</td>\n",
       "      <td>-0.012557</td>\n",
       "      <td>-0.033914</td>\n",
       "      <td>0.025563</td>\n",
       "      <td>0.049251</td>\n",
       "      <td>-0.017813</td>\n",
       "      <td>-0.048582</td>\n",
       "      <td>0.007284</td>\n",
       "      <td>-0.007794</td>\n",
       "      <td>-0.012504</td>\n",
       "      <td>-0.019073</td>\n",
       "      <td>0.378669</td>\n",
       "      <td>0.377172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.340912</td>\n",
       "      <td>0.599515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M3P0</th>\n",
       "      <td>0.015417</td>\n",
       "      <td>0.004526</td>\n",
       "      <td>-0.041641</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>-0.061581</td>\n",
       "      <td>0.039178</td>\n",
       "      <td>-0.028942</td>\n",
       "      <td>-0.031852</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>-0.019744</td>\n",
       "      <td>-0.009929</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>-0.011986</td>\n",
       "      <td>0.010490</td>\n",
       "      <td>-0.014380</td>\n",
       "      <td>0.334359</td>\n",
       "      <td>0.349248</td>\n",
       "      <td>0.340912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.572507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>0.004537</td>\n",
       "      <td>-0.004630</td>\n",
       "      <td>-0.005575</td>\n",
       "      <td>0.018090</td>\n",
       "      <td>-0.056799</td>\n",
       "      <td>0.031985</td>\n",
       "      <td>-0.015338</td>\n",
       "      <td>-0.010288</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.033218</td>\n",
       "      <td>-0.019832</td>\n",
       "      <td>-0.037189</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>-0.010911</td>\n",
       "      <td>-0.030721</td>\n",
       "      <td>0.595758</td>\n",
       "      <td>0.594514</td>\n",
       "      <td>0.599515</td>\n",
       "      <td>0.572507</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             N0        N1        N2        N3        N4        N5        N6  \\\n",
       "N0     1.000000  0.028505  0.027041  0.017031 -0.012241  0.000403  0.002018   \n",
       "N1     0.028505  1.000000  0.010472  0.045569  0.008949 -0.009164  0.044185   \n",
       "N2     0.027041  0.010472  1.000000 -0.023226  0.046411  0.039052  0.014061   \n",
       "N3     0.017031  0.045569 -0.023226  1.000000 -0.022348  0.005947 -0.005448   \n",
       "N4    -0.012241  0.008949  0.046411 -0.022348  1.000000 -0.017326 -0.014026   \n",
       "N5     0.000403 -0.009164  0.039052  0.005947 -0.017326  1.000000  0.011501   \n",
       "N6     0.002018  0.044185  0.014061 -0.005448 -0.014026  0.011501  1.000000   \n",
       "N7     0.010503 -0.043989 -0.019100  0.042595  0.000841  0.070218  0.009042   \n",
       "N8     0.016489 -0.022059  0.051320  0.024844  0.008281  0.019938 -0.020355   \n",
       "N9     0.009598  0.017311 -0.035171 -0.062843  0.030221  0.022306 -0.024109   \n",
       "N10    0.054169  0.010628  0.036079  0.025329 -0.040326  0.023822 -0.006838   \n",
       "N11   -0.027643 -0.022036 -0.007569  0.009478 -0.019487  0.030982  0.000114   \n",
       "N12    0.020725 -0.026798 -0.014152 -0.028009  0.014793 -0.008791  0.031939   \n",
       "N13    0.006256 -0.042155  0.020177  0.047040 -0.010957 -0.016136  0.014276   \n",
       "N14    0.036304  0.009564 -0.026440 -0.010527  0.029609  0.014218 -0.062328   \n",
       "N15   -0.009907 -0.032048  0.017467  0.003434  0.038077 -0.010957 -0.005975   \n",
       "M0P0   0.008227 -0.014438 -0.014845 -0.012027 -0.034656  0.005830 -0.005174   \n",
       "M1P0   0.019241  0.007653 -0.014296  0.009476 -0.033374  0.032210 -0.004982   \n",
       "M2P0   0.006094  0.007829 -0.007439 -0.004956 -0.033258  0.030356 -0.012557   \n",
       "M3P0   0.015417  0.004526 -0.041641  0.007965 -0.061581  0.039178 -0.028942   \n",
       "Class  0.004537 -0.004630 -0.005575  0.018090 -0.056799  0.031985 -0.015338   \n",
       "\n",
       "             N7        N8        N9       N10       N11       N12       N13  \\\n",
       "N0     0.010503  0.016489  0.009598  0.054169 -0.027643  0.020725  0.006256   \n",
       "N1    -0.043989 -0.022059  0.017311  0.010628 -0.022036 -0.026798 -0.042155   \n",
       "N2    -0.019100  0.051320 -0.035171  0.036079 -0.007569 -0.014152  0.020177   \n",
       "N3     0.042595  0.024844 -0.062843  0.025329  0.009478 -0.028009  0.047040   \n",
       "N4     0.000841  0.008281  0.030221 -0.040326 -0.019487  0.014793 -0.010957   \n",
       "N5     0.070218  0.019938  0.022306  0.023822  0.030982 -0.008791 -0.016136   \n",
       "N6     0.009042 -0.020355 -0.024109 -0.006838  0.000114  0.031939  0.014276   \n",
       "N7     1.000000 -0.012661  0.005445 -0.004711  0.012266  0.012939 -0.040074   \n",
       "N8    -0.012661  1.000000 -0.006873  0.003657  0.009244 -0.011400  0.007191   \n",
       "N9     0.005445 -0.006873  1.000000  0.024714 -0.018259 -0.014836 -0.030787   \n",
       "N10   -0.004711  0.003657  0.024714  1.000000 -0.015393 -0.017048 -0.014114   \n",
       "N11    0.012266  0.009244 -0.018259 -0.015393  1.000000 -0.010897 -0.074383   \n",
       "N12    0.012939 -0.011400 -0.014836 -0.017048 -0.010897  1.000000 -0.020675   \n",
       "N13   -0.040074  0.007191 -0.030787 -0.014114 -0.074383 -0.020675  1.000000   \n",
       "N14   -0.007484 -0.039169 -0.002289  0.013560  0.047419 -0.052550 -0.018908   \n",
       "N15    0.037897 -0.009993  0.006376 -0.056905  0.030401  0.011137  0.003342   \n",
       "M0P0   0.000242 -0.007004 -0.018658 -0.008082 -0.050507 -0.043890  0.022353   \n",
       "M1P0   0.000233 -0.016353  0.032912  0.008707 -0.008622 -0.000914 -0.012063   \n",
       "M2P0  -0.033914  0.025563  0.049251 -0.017813 -0.048582  0.007284 -0.007794   \n",
       "M3P0  -0.031852  0.000578  0.022624 -0.019744 -0.009929  0.007487 -0.011986   \n",
       "Class -0.010288  0.001156  0.033218 -0.019832 -0.037189  0.002618  0.008782   \n",
       "\n",
       "            N14       N15      M0P0      M1P0      M2P0      M3P0     Class  \n",
       "N0     0.036304 -0.009907  0.008227  0.019241  0.006094  0.015417  0.004537  \n",
       "N1     0.009564 -0.032048 -0.014438  0.007653  0.007829  0.004526 -0.004630  \n",
       "N2    -0.026440  0.017467 -0.014845 -0.014296 -0.007439 -0.041641 -0.005575  \n",
       "N3    -0.010527  0.003434 -0.012027  0.009476 -0.004956  0.007965  0.018090  \n",
       "N4     0.029609  0.038077 -0.034656 -0.033374 -0.033258 -0.061581 -0.056799  \n",
       "N5     0.014218 -0.010957  0.005830  0.032210  0.030356  0.039178  0.031985  \n",
       "N6    -0.062328 -0.005975 -0.005174 -0.004982 -0.012557 -0.028942 -0.015338  \n",
       "N7    -0.007484  0.037897  0.000242  0.000233 -0.033914 -0.031852 -0.010288  \n",
       "N8    -0.039169 -0.009993 -0.007004 -0.016353  0.025563  0.000578  0.001156  \n",
       "N9    -0.002289  0.006376 -0.018658  0.032912  0.049251  0.022624  0.033218  \n",
       "N10    0.013560 -0.056905 -0.008082  0.008707 -0.017813 -0.019744 -0.019832  \n",
       "N11    0.047419  0.030401 -0.050507 -0.008622 -0.048582 -0.009929 -0.037189  \n",
       "N12   -0.052550  0.011137 -0.043890 -0.000914  0.007284  0.007487  0.002618  \n",
       "N13   -0.018908  0.003342  0.022353 -0.012063 -0.007794 -0.011986  0.008782  \n",
       "N14    1.000000  0.015323 -0.014445  0.051107 -0.012504  0.010490 -0.010911  \n",
       "N15    0.015323  1.000000 -0.019667 -0.004343 -0.019073 -0.014380 -0.030721  \n",
       "M0P0  -0.014445 -0.019667  1.000000  0.334838  0.378669  0.334359  0.595758  \n",
       "M1P0   0.051107 -0.004343  0.334838  1.000000  0.377172  0.349248  0.594514  \n",
       "M2P0  -0.012504 -0.019073  0.378669  0.377172  1.000000  0.340912  0.599515  \n",
       "M3P0   0.010490 -0.014380  0.334359  0.349248  0.340912  1.000000  0.572507  \n",
       "Class -0.010911 -0.030721  0.595758  0.594514  0.599515  0.572507  1.000000  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = raw_data.iloc[:, -1].values\n",
    "X = raw_data.iloc[:, :-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a 2-layer neural network with one input layer, one hidden layer, and one output layer (note that the input layer is typically excluded when counting the number of layers in a Neural Network). <br><br>\n",
    "The number of nodes in the input layer is equal to the number of features (columns) in my data, 20. <br>\n",
    "The number of nodes in the output layer is determined by the number of classes we have, 2. <br>\n",
    "As for the number of nodes in the hidden layer, we can choose by ourselves. For most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules: (i) number of hidden layers equals one; and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers. Thus, this time, I will set the number of nodes in the hidden layer to be 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need an activation function for our hidden layer. The activation function of a node defines the output of that node given an input or set of inputs.A nonlinear activation function is what allows us to fit nonlinear hypotheses. Common chocies for activation functions are tanh, the sigmoid function and ReLUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](tanh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use tanh in this notebook. It is a rescaling of the logistic sigmoid such that it squashes a number to the range [-1, 1] and its output is zero-centered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the output layer, the activation function will be the softmax function which represents a categorical distribution – that is, a probability distribution over K different possible outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](2l.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the right values for the weights and biases determines the strength of the predictions. The process of fine-tuning the weights and biases from the input data is known as training the Neural Network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration of the training process consists of the following steps:<br>\n",
    "Calculating the predicted output ŷ, known as feedforward<br>\n",
    "Updating the weights and biases, known as backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The output ŷ of a simple 2-layer Neural Network is:<br>\n",
    "\\begin{aligned}\n",
    "z_1 & = xW_1 + b_1 \\\\\n",
    "a_1 & = \\tanh(z_1) \\\\\n",
    "z_2 & = a_1W_2 + b_2 \\\\\n",
    "a_2 & = \\hat{y} = \\mathrm{softmax}(z_2)\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The weights W and the biases b are the only variables that affects the output ŷ.<br>\n",
    " W1,b1,W2,b2 are parameters of our network that we need to learn from our training data.Our goal is to find parameters (W1,b1,W2,b2) that minimize the error on our training data. We call the function that measures our error the loss function. A common choice with the softmax output is the cross-entropy loss. If we have N training examples and C classes, then the loss for our prediction ŷ with respect to the true labels  y\n",
    "  is given by:<br>\n",
    " \n",
    "\\begin{aligned}\n",
    "L(y,\\hat{y}) = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\hat{y}_{n,i}\n",
    "\\end{aligned}\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in training is to find the best set of parameters that minimizes the loss function.Now that we’ve measured the loss, we want to propagation the error back and to update our weights and biases. To do these, We need to know the derivative of the loss function with respect to the weights and biases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](gd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update weights and biases by increasing/reducing with the derivative(refer to the diagram above) . This is known as gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use backpropagation algorithm to calculate the gradients starting from the output. \n",
    "\n",
    "We have the following backpropagation formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& \\delta_3 = \\hat{y} - y \\\\\n",
    "& \\delta_2 = (1 - \\tanh^2z_1) \\circ \\delta_3W_2^T \\\\\n",
    "& \\frac{\\partial{L}}{\\partial{W_2}} = a_1^T \\delta_3  \\\\\n",
    "& \\frac{\\partial{L}}{\\partial{b_2}} = \\delta_3\\\\\n",
    "& \\frac{\\partial{L}}{\\partial{W_1}} = x^T \\delta_2\\\\\n",
    "& \\frac{\\partial{L}}{\\partial{b_1}} = \\delta_2 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural network structure\n",
    "num_input_layer = 20 # number of nodes in input layer\n",
    "num_output_layer = 2 # number of nodes in output layer\n",
    "num_hidden_layer = 11 # number of nodes in hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient descent parameters \n",
    "learning_rate = 0.001\n",
    "regularization_lambda = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function predicts output(0 or 1).It calculate the output of the neural network and return the class with highest probability. \n",
    "def predict(m, x):\n",
    "    w1, w2, b1, b2 = m['w1'],m['w2'],m['b1'],m['b2']\n",
    "    \n",
    "    # feedforward\n",
    "    z1 = x.dot(w1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(w2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) \n",
    "    return np.argmax(probs, axis=1)# np.argmax returns the indices of the maximum values along an axis.Axis = 1, max in each row; axis = 0, max in each column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This functions calculates total loss\n",
    "def total_loss(m, X = X, y = y):\n",
    "    w1, w2, b1, b2 = m['w1'],m['w2'],m['b1'],m['b2']\n",
    "    train_size = len(X)\n",
    "    \n",
    "    # feedforward\n",
    "    z1 = X.dot(w1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(w2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss_sum = np.sum(-np.log(probs[range(train_size), y]))\n",
    "    loss  = 1.0/train_size * loss_sum\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function returns a neural net work model with learned parameters\n",
    "# num_n_hidden: number of nodes in hidden layer\n",
    "# num_iteration: number of passes through the training data for gradient descent\n",
    "# print_loss: print the loss every 1000 iterations when True\n",
    "def nn_model( num_n_hidden, num_iteration,learning_rate,  print_loss = False, X = X, y = y):\n",
    "    # we will return a model with parameters at the end: \n",
    "    model = {}\n",
    "    train_size = len(X)\n",
    "    \n",
    "    # initialize parameters \n",
    "    np.random.seed(39)\n",
    "    w1 = np.random.randn(num_input_layer , num_n_hidden) / np.sqrt(num_input_layer )\n",
    "    b1 = np.zeros((1, num_n_hidden))\n",
    "    w2 = np.random.randn(num_n_hidden, num_output_layer) / np.sqrt(num_n_hidden)\n",
    "    b2 = np.zeros((1, num_output_layer))\n",
    "    \n",
    "    # gradient descent\n",
    "    for i in range(0, num_iteration):\n",
    "        # feedforward\n",
    "        z1 = X.dot(w1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(w2) + b2\n",
    "        exp_scores = np.exp(z2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        # backpropagation\n",
    "        delta3 = probs\n",
    "        delta3[range(train_size), y] -= 1\n",
    "        dw2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(w2.T) * (1 - np.power(a1, 2))\n",
    "        dw1 = np.dot(X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dw2 += regularization_lambda * w2\n",
    "        dw1 += regularization_lambda * w1\n",
    "        # Gradient descent parameter update\n",
    "        w1 += -learning_rate * dw1\n",
    "        b1 += -learning_rate * db1\n",
    "        w2 += -learning_rate * dw2\n",
    "        b2 += -learning_rate * db2\n",
    "        \n",
    "        model = {'w1':w1, 'w2':w2, 'b1':b1, 'b2':b2}\n",
    "        \n",
    "        #print loss\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print('Loss after %i iteration:%f' %(i,total_loss(model,X = X, y = y)))\n",
    "   \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function does 10-fold. It saves the result at each time as different parts of y_pred. \n",
    "# In the end, it returns the y_pred as the result of all the 10-fold.\n",
    "def run_cv(X,y, num_hidden_layer,num_iteration,learning_rate):\n",
    "    # Construct a kfolds object\n",
    "    kf = KFold(len(y),n_folds=10,shuffle=True) # Total number of elements；Number of folds， default=3；Whether to shuffle the data before splitting into batches\n",
    "    y_pred = y.copy()\n",
    "     \n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        \n",
    "        m = nn_model(num_hidden_layer,num_iteration, learning_rate,print_loss = False,X = X_train, y = y_train)\n",
    "        y_pred[test_index] = predict(m, X_test)\n",
    "         \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a neural network with a hidden layer size of 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration:0.554981\n",
      "Loss after 1000 iteration:0.072043\n",
      "Loss after 2000 iteration:0.032190\n",
      "Loss after 3000 iteration:0.026320\n",
      "Loss after 4000 iteration:0.016058\n",
      "Loss after 5000 iteration:0.011195\n",
      "Loss after 6000 iteration:0.008339\n",
      "Loss after 7000 iteration:0.006668\n",
      "Loss after 8000 iteration:0.005636\n",
      "Loss after 9000 iteration:0.004271\n",
      "Loss after 10000 iteration:0.003611\n",
      "Loss after 11000 iteration:0.003128\n",
      "Loss after 12000 iteration:0.002779\n",
      "Loss after 13000 iteration:0.002510\n",
      "Loss after 14000 iteration:0.002288\n",
      "Loss after 15000 iteration:0.002097\n",
      "Loss after 16000 iteration:0.001931\n",
      "Loss after 17000 iteration:0.001791\n",
      "Loss after 18000 iteration:0.001675\n",
      "Loss after 19000 iteration:0.001578\n"
     ]
    }
   ],
   "source": [
    "# with learning_rate = 0.001\n",
    "model = nn_model(num_hidden_layer,20000, learning_rate,print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = run_cv(X,y,num_hidden_layer,20000, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function calculates accuracy\n",
    "def accuracy(y_true,y_pred):\n",
    "    return np.mean(y_true == y_pred) # NumPy interpretes True and False as 1. and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy: 0.89875\n"
     ]
    }
   ],
   "source": [
    "print('Neural network accuracy: ' + str(accuracy(y, res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Change number of iterations\n",
    "\n",
    "We have run 20000 iterations in the previous example. We want to change iteration number and to see how it affects result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 2000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration:0.554981\n",
      "Loss after 1000 iteration:0.072043\n"
     ]
    }
   ],
   "source": [
    "# with learning_rate = 0.001\n",
    "model_i_2000 = nn_model(num_hidden_layer,2000, learning_rate,print_loss = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_i_2000 = run_cv(X,y,num_hidden_layer,2000, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy: 0.906875\n"
     ]
    }
   ],
   "source": [
    "print('Neural network accuracy: ' + str(accuracy(y, res_i_2000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 5000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration:0.554981\n",
      "Loss after 1000 iteration:0.072043\n",
      "Loss after 2000 iteration:0.032190\n",
      "Loss after 3000 iteration:0.026320\n",
      "Loss after 4000 iteration:0.016058\n"
     ]
    }
   ],
   "source": [
    "# with learning_rate = 0.001\n",
    "model_i_5000 = nn_model(num_hidden_layer,5000, learning_rate,print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_i_5000 = run_cv(X,y,num_hidden_layer,5000, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy: 0.899375\n"
     ]
    }
   ],
   "source": [
    "print('Neural network accuracy: ' + str(accuracy(y, res_i_5000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration:0.554981\n",
      "Loss after 1000 iteration:0.072043\n",
      "Loss after 2000 iteration:0.032190\n",
      "Loss after 3000 iteration:0.026320\n",
      "Loss after 4000 iteration:0.016058\n",
      "Loss after 5000 iteration:0.011195\n",
      "Loss after 6000 iteration:0.008339\n",
      "Loss after 7000 iteration:0.006668\n",
      "Loss after 8000 iteration:0.005636\n",
      "Loss after 9000 iteration:0.004271\n"
     ]
    }
   ],
   "source": [
    "# with learning_rate = 0.001\n",
    "model_i_10000 = nn_model(num_hidden_layer,10000, learning_rate,print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_i_10000 = run_cv(X,y,num_hidden_layer,10000, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy: 0.896875\n"
     ]
    }
   ],
   "source": [
    "print('Neural network accuracy: ' + str(accuracy(y, res_i_10000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that large iteration numbers do not lead to high accuracy probably due to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change learning rate\n",
    "\n",
    "We used learning rate 0.001 for the previous examples. We will change it and see how it affects result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set learning rate to be 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration:2.029559\n",
      "Loss after 1000 iteration:85.907363\n",
      "Loss after 2000 iteration:82.080926\n",
      "Loss after 3000 iteration:5.449913\n",
      "Loss after 4000 iteration:8.598803\n",
      "Loss after 5000 iteration:7.837303\n",
      "Loss after 6000 iteration:10.955712\n",
      "Loss after 7000 iteration:4.772520\n",
      "Loss after 8000 iteration:9.626135\n",
      "Loss after 9000 iteration:3.823448\n",
      "Loss after 10000 iteration:2.757594\n",
      "Loss after 11000 iteration:8.092454\n",
      "Loss after 12000 iteration:3.321667\n",
      "Loss after 13000 iteration:6.381027\n",
      "Loss after 14000 iteration:5.121073\n",
      "Loss after 15000 iteration:3.347808\n",
      "Loss after 16000 iteration:0.781658\n",
      "Loss after 17000 iteration:7.170488\n",
      "Loss after 18000 iteration:4.617957\n",
      "Loss after 19000 iteration:38.649404\n"
     ]
    }
   ],
   "source": [
    "model_lr_01 = nn_model(num_hidden_layer,20000, 0.01,print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_lr_01 = run_cv(X,y,num_hidden_layer,20000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy: 0.77375\n"
     ]
    }
   ],
   "source": [
    "print('Neural network accuracy: ' + str(accuracy(y, res_lr_01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set learning rate to be 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration:0.690073\n",
      "Loss after 1000 iteration:4.759541\n",
      "Loss after 2000 iteration:0.892556\n",
      "Loss after 3000 iteration:0.480978\n",
      "Loss after 4000 iteration:1.079858\n",
      "Loss after 5000 iteration:1.621521\n",
      "Loss after 6000 iteration:0.750379\n",
      "Loss after 7000 iteration:1.166582\n",
      "Loss after 8000 iteration:1.251452\n",
      "Loss after 9000 iteration:1.042488\n",
      "Loss after 10000 iteration:1.030003\n",
      "Loss after 11000 iteration:1.159303\n",
      "Loss after 12000 iteration:2.470673\n",
      "Loss after 13000 iteration:0.580804\n",
      "Loss after 14000 iteration:0.999351\n",
      "Loss after 15000 iteration:0.619001\n",
      "Loss after 16000 iteration:13.594964\n",
      "Loss after 17000 iteration:0.872865\n",
      "Loss after 18000 iteration:3.440464\n",
      "Loss after 19000 iteration:8.103656\n"
     ]
    }
   ],
   "source": [
    "model_lr_005 = nn_model(num_hidden_layer,20000, 0.005,print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_lr_005 = run_cv(X,y, num_hidden_layer,20000, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy: 0.890625\n"
     ]
    }
   ],
   "source": [
    "print('Neural network accuracy: ' + str(accuracy(y, res_lr_005)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set learning rate to be 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration:0.720266\n",
      "Loss after 1000 iteration:0.161367\n",
      "Loss after 2000 iteration:0.117620\n",
      "Loss after 3000 iteration:0.098035\n",
      "Loss after 4000 iteration:0.087000\n",
      "Loss after 5000 iteration:0.078478\n",
      "Loss after 6000 iteration:0.070033\n",
      "Loss after 7000 iteration:0.061677\n",
      "Loss after 8000 iteration:0.054112\n",
      "Loss after 9000 iteration:0.047605\n",
      "Loss after 10000 iteration:0.042019\n",
      "Loss after 11000 iteration:0.037301\n",
      "Loss after 12000 iteration:0.033397\n",
      "Loss after 13000 iteration:0.030184\n",
      "Loss after 14000 iteration:0.027514\n",
      "Loss after 15000 iteration:0.025248\n",
      "Loss after 16000 iteration:0.023274\n",
      "Loss after 17000 iteration:0.021536\n",
      "Loss after 18000 iteration:0.020000\n",
      "Loss after 19000 iteration:0.018638\n"
     ]
    }
   ],
   "source": [
    "model_lr_0001 = nn_model(num_hidden_layer,20000, 0.0001,print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_lr_0001 = run_cv(X,y,num_hidden_layer,20000, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy: 0.896875\n"
     ]
    }
   ],
   "source": [
    "print('Neural network accuracy: ' + str(accuracy(y, res_lr_0001)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.\n",
    " \n",
    " We can try several values and find the one with best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change hidden layer size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the hidden layer size of 11 for previous examples. Now we want to pick different hidden layer sizes and to see how does hidden layer size affects results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hidden layer size to be 1, 5, 10, 15 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer = [1, 5, 10, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy with hidden layer size 1 is: 0.8975\n",
      "\n",
      "\n",
      "Neural network accuracy with hidden layer size 5 is: 0.896875\n",
      "\n",
      "\n",
      "Neural network accuracy with hidden layer size 10 is: 0.89125\n",
      "\n",
      "\n",
      "Neural network accuracy with hidden layer size 15 is: 0.898125\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer_size in hidden_layer:\n",
    "     \n",
    "    res_hl = run_cv(X_v,y_v,layer_size,20000, 0.001)\n",
    "    \n",
    "    print('Neural network accuracy with hidden layer size ' + str(layer_size) + ' is: ' + str(accuracy(y, res_hl)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: <br>\n",
    "https://en.wikipedia.org/wiki/Artificial_neural_network<br>\n",
    "https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw<br>\n",
    "https://en.wikipedia.org/wiki/Activation_function<br>\n",
    "https://en.wikipedia.org/wiki/Softmax_function<br>\n",
    "https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6<br>\n",
    "https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression<br>\n",
    "http://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/<br>\n",
    "http://theory.stanford.edu/~tim/s16/l/l6.pdf<br>\n",
    "https://towardsdatascience.com/random-initialization-for-neural-networks-a-thing-of-the-past-bfcdd806bf9e<br>\n",
    "http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
