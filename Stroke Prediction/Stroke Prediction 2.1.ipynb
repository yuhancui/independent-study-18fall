{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stroke Prediction 2.1: Model and evaulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)# display all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"clean_train.csv\")\n",
    "test_data = pd.read_csv(\"clean_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>Other</th>\n",
       "      <th>Govt_job</th>\n",
       "      <th>Never_worked</th>\n",
       "      <th>Private</th>\n",
       "      <th>Self-employed</th>\n",
       "      <th>children</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.12</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.96</td>\n",
       "      <td>39.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.89</td>\n",
       "      <td>17.6</td>\n",
       "      <td>0.326663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.04</td>\n",
       "      <td>35.9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.28</td>\n",
       "      <td>19.1</td>\n",
       "      <td>0.681701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  hypertension  heart_disease  ever_married  avg_glucose_level   bmi  \\\n",
       "0   3.0           0.0            0.0           0.0              95.12  18.0   \n",
       "1  58.0           1.0            0.0           1.0              87.96  39.2   \n",
       "2   8.0           0.0            0.0           0.0             110.89  17.6   \n",
       "3  70.0           0.0            0.0           1.0              69.04  35.9   \n",
       "4  14.0           0.0            0.0           0.0             161.28  19.1   \n",
       "\n",
       "   smoking_status  Female  Male  Other  Govt_job  Never_worked  Private  \\\n",
       "0        0.000000     0.0   1.0    0.0       0.0           0.0      0.0   \n",
       "1        0.000000     0.0   1.0    0.0       0.0           0.0      1.0   \n",
       "2        0.326663     1.0   0.0    0.0       0.0           0.0      1.0   \n",
       "3        1.000000     1.0   0.0    0.0       0.0           0.0      1.0   \n",
       "4        0.681701     0.0   1.0    0.0       0.0           1.0      0.0   \n",
       "\n",
       "   Self-employed  children  stroke  \n",
       "0            0.0       1.0     0.0  \n",
       "1            0.0       0.0     0.0  \n",
       "2            0.0       0.0     0.0  \n",
       "3            0.0       0.0     0.0  \n",
       "4            0.0       0.0     0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_data.iloc[:, :-1].values\n",
    "y = train_data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale the data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemable learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common ensemble techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking<br><br>\n",
    "Stacking (sometimes called stacked generalization) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs.<br><br>\n",
    "Blending<br><br>\n",
    "Blending follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions. In other words, unlike stacking, the predictions are made on the holdout set only. The holdout set and the predictions are used to build a model which is run on the test set.<br><br>\n",
    "Bagging<br><br>\n",
    "The idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result.<br><br>\n",
    "Boosting<br><br>\n",
    "Boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified. In some cases, boosting has been shown to yield better accuracy than bagging, but it also tends to be more likely to over-fit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms based on Bagging and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging algorithms:<br><br>\n",
    "Bagging meta-estimator: Bagging meta-estimator is an ensembling algorithm that can be used for both classification (BaggingClassifier) and regression (BaggingRegressor) problems. It follows the typical bagging technique to make predictions. The subset of the dataset includes all features.<br><br>\n",
    "Random forest:It is an extension of the bagging estimator algorithm. The base estimators in random forest are decision trees. Unlike bagging meta estimator, random forest randomly selects a set of features which are used to decide the best split at each node of the decision tree.<br><br>\n",
    "Boosting algorithms:<br><br>\n",
    "AdaBoost：Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.<br><br>\n",
    "GBM：Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree.<br><br>\n",
    "XGBM：XGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as ‘regularized boosting‘ technique.<br><br>\n",
    "Light GBM：Light GBM beats all the other algorithms when the dataset is extremely large. Compared to the other algorithms, Light GBM takes lesser time to run on a huge dataset.LightGBM is a gradient boosting framework that uses tree-based algorithms and follows leaf-wise approach while other algorithms work in a level-wise approach pattern.<br><br>\n",
    "CatBoost：CatBoost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter sweeping and model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging meta-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging meta-estimator accuracy:  0.9813594411046613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print (\"Bagging meta-estimator accuracy: \", np.mean(cross_val_score(BaggingClassifier(), X, y,cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_esti = [5, 10, 15, 20, 25, 30]\n",
    "max_samp = [1, 2, 3, 4, 5]\n",
    "max_feat = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_res = []\n",
    "for ne in n_esti:\n",
    "    for ms in max_samp:\n",
    "        for mf in max_feat:\n",
    "            score = np.mean(cross_val_score(BaggingClassifier(n_estimators = ne, max_samples = ms, max_features = mf), X, y,cv=10))\n",
    "            bag_res.append([score, ne, ms, mf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find result with highest accuracy\n",
    "def find_best(lst):\n",
    "    return max(lst, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After parameter sweeping, we find:\n",
      "Highest accuracy is: 0.981958540696738 with n_estimators  5 , max_samples  1  and max_features  1\n"
     ]
    }
   ],
   "source": [
    "print ('After parameter sweeping, we find:')\n",
    "print ('Highest accuracy is:', find_best(bag_res)[0], 'with n_estimators ', find_best(bag_res)[1], ', max_samples ', find_best(bag_res)[2], ' and max_features ',find_best(bag_res)[3] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy:  0.9814285761443695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print (\"Random forest accuracy: \",np.mean(cross_val_score(RandomForestClassifier(), X, y,cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_samples_leaf=[1, 4, 7, 11, 14, 17] \n",
    "n_estimators=[1, 6, 11, 16, 21,26, 31, 36, 41, 46] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_res = []\n",
    "for ms in min_samples_leaf:\n",
    "    for ne in n_estimators:\n",
    "        score = np.mean(cross_val_score(RandomForestClassifier(n_estimators = ne,min_samples_leaf = ms), X, y, cv=10))\n",
    "        rf_res.append([score,ms,ne])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After parameter sweeping, we find:\n",
      "Highest accuracy is: 0.9819815768635198 min_samples_leaf  4 and n_estimators 16\n"
     ]
    }
   ],
   "source": [
    "print ('After parameter sweeping, we find:')\n",
    "print ('Highest accuracy is:', find_best(rf_res)[0], 'min_samples_leaf ', find_best(rf_res)[1], 'and n_estimators', find_best(rf_res)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost accuracy:  0.9819124683631744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "print (\"AdaBoost accuracy: \",np.mean(cross_val_score(AdaBoostClassifier(), X, y,cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_esti = [30, 40, 50, 60, 70]\n",
    "lr = [1, 0.5, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ada_res = []\n",
    "for ne in n_esti:\n",
    "    for l in lr:\n",
    "        score = np.mean(cross_val_score(AdaBoostClassifier(n_estimators = ne, learning_rate = l), X, y,cv=10))\n",
    "        ada_res.append([score, ne, l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After parameter sweeping, we find:\n",
      "Highest accuracy is: 0.981958540696738 n_estimators  30 and learning_rate 1\n"
     ]
    }
   ],
   "source": [
    "print ('After parameter sweeping, we find:')\n",
    "print ('Highest accuracy is:', find_best(ada_res)[0], 'n_estimators ', find_best(ada_res)[1], 'and learning_rate', find_best(ada_res)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting accuracy:  0.9815668196844234\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "print (\"Gradient Boosting accuracy: \",np.mean(cross_val_score(GradientBoostingClassifier(), X, y,cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_esti = [90, 100, 110]\n",
    "lr = [1, 0.5, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_res = []\n",
    "for ne in n_esti:\n",
    "    for l in lr:\n",
    "        score = np.mean(cross_val_score(GradientBoostingClassifier(n_estimators = ne, learning_rate = l), X, y,cv=10))\n",
    "        gb_res.append([score, ne, l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After parameter sweeping, we find:\n",
      "Highest accuracy is: 0.9816359441108332 n_estimators  90 and learning_rate 0.1\n"
     ]
    }
   ],
   "source": [
    "print ('After parameter sweeping, we find:')\n",
    "print ('Highest accuracy is:', find_best(gb_res)[0], 'n_estimators ', find_best(gb_res)[1], 'and learning_rate', find_best(gb_res)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy:  0.981958540696738\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "print (\"SVM accuracy: \",np.mean(cross_val_score(SVC(), X, y,cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_res = []\n",
    "for k in kernels:\n",
    "    score = np.mean(cross_val_score(SVC(kernel=k), X, y,cv=10))\n",
    "    svm_res.append([score,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After parameter sweeping, we find:\n",
      "Highest accuracy is: 0.981958540696738 kernel  linear\n"
     ]
    }
   ],
   "source": [
    "print ('After parameter sweeping, we find:')\n",
    "print ('Highest accuracy is:', find_best(svm_res)[0], 'kernel ', find_best(svm_res)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy:  0.981958540696738\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print (\"Logistic Regression accuracy: \",np.mean(cross_val_score(LogisticRegression(), X, y,cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "penalty = ['l1', 'l2']\n",
    "C = [1, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lg_res = []\n",
    "for p in penalty:\n",
    "    for c in C:\n",
    "        score = np.mean(cross_val_score(LogisticRegression(C = c, penalty=p), X, y, cv=10))\n",
    "        lg_res.append([score, p, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After parameter sweeping, we find:\n",
      "Highest accuracy is: 0.981958540696738 with penalty  l1 and C  1\n"
     ]
    }
   ],
   "source": [
    "print ('After parameter sweeping, we find:')\n",
    "print ('Highest accuracy is:', find_best(lg_res)[0], 'with penalty ', find_best(lg_res)[1], 'and C ', find_best(lg_res)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC score  & Classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging meta-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging meta-estimator auc score: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Bagging meta-estimator auc score:\",np.mean(cross_val_score(BaggingClassifier(n_estimators = 5, max_samples = 1,max_features = 1), X, y, scoring = 'roc_auc',cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    originalclass.extend(y_true)\n",
    "    predictedclass.extend(y_pred)\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     42617\n",
      "         1.0       0.00      0.00      0.00       783\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     43400\n",
      "   macro avg       0.49      0.50      0.50     43400\n",
      "weighted avg       0.96      0.98      0.97     43400\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "nested_score = cross_val_score(BaggingClassifier(n_estimators = 5, max_samples = 1,max_features = 1), X, y, cv=10,\\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(classification_report(originalclass, predictedclass)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest auc score: 0.8079906216569255\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest auc score:\",np.mean(cross_val_score(RandomForestClassifier(n_estimators = 16,min_samples_leaf = 4), X, y, scoring = 'roc_auc',cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     42617\n",
      "         1.0       1.00      0.00      0.00       783\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     43400\n",
      "   macro avg       0.99      0.50      0.50     43400\n",
      "weighted avg       0.98      0.98      0.97     43400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "originalclass = []\n",
    "predictedclass = []\n",
    "nested_score = cross_val_score(RandomForestClassifier(n_estimators = 16,min_samples_leaf = 4), X, y, cv=10,\\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(classification_report(originalclass, predictedclass)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost auc score: 0.8465747519482829\n"
     ]
    }
   ],
   "source": [
    "print(\"AdaBoost auc score:\",np.mean(cross_val_score(AdaBoostClassifier(n_estimators =30, learning_rate = 1), X, y, scoring = 'roc_auc',cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     42617\n",
      "         1.0       0.00      0.00      0.00       783\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     43400\n",
      "   macro avg       0.49      0.50      0.50     43400\n",
      "weighted avg       0.96      0.98      0.97     43400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "originalclass = []\n",
    "predictedclass = []\n",
    "nested_score = cross_val_score(AdaBoostClassifier(n_estimators =30, learning_rate = 1), X, y, cv=10,\\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(classification_report(originalclass, predictedclass)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting auc score: 0.8495456612215481\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Boosting auc score:\",np.mean(cross_val_score(GradientBoostingClassifier(n_estimators =90,learning_rate = 0.1), X, y, scoring = 'roc_auc',cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     42617\n",
      "         1.0       0.00      0.00      0.00       783\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     43400\n",
      "   macro avg       0.49      0.50      0.50     43400\n",
      "weighted avg       0.96      0.98      0.97     43400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "originalclass = []\n",
    "predictedclass = []\n",
    "nested_score = cross_val_score(GradientBoostingClassifier(n_estimators =90,learning_rate = 0.1), X, y, cv=10,\\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(classification_report(originalclass, predictedclass)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM auc score: 0.5403611184241444\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM auc score:\",np.mean(cross_val_score(SVC(kernel = 'linear'), X, y, scoring = 'roc_auc',cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     42617\n",
      "         1.0       0.00      0.00      0.00       783\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     43400\n",
      "   macro avg       0.49      0.50      0.50     43400\n",
      "weighted avg       0.96      0.98      0.97     43400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "originalclass = []\n",
    "predictedclass = []\n",
    "nested_score = cross_val_score(SVC(kernel = 'linear'), X, y, cv=10,\\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(classification_report(originalclass, predictedclass)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression auc score: 0.8509406001768003\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression auc score:\",np.mean(cross_val_score(LogisticRegression(C = 1, penalty='l1'), X, y, scoring = 'roc_auc',cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     42617\n",
      "         1.0       0.00      0.00      0.00       783\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     43400\n",
      "   macro avg       0.49      0.50      0.50     43400\n",
      "weighted avg       0.96      0.98      0.97     43400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "originalclass = []\n",
    "predictedclass = []\n",
    "nested_score = cross_val_score(LogisticRegression(C = 1, penalty='l1'), X, y, cv=10,\\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(classification_report(originalclass, predictedclass)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commom ways include:<br><br>\n",
    "1.Max Voting<br><br>\n",
    "The max voting method is generally used for classification problems. Here, multiple models are used to make predictions for each data point and the predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction.<br><br>\n",
    "2.Averaging<br><br>\n",
    "In this method, we take an average of predictions from all the models and use it to make the final prediction.<br><br>\n",
    "3.Weighted Averaging<br><br>\n",
    "This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction.<br><br>\n",
    "In this project, I will use max voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = LogisticRegression(C = 1, penalty='l1')\n",
    "model2 = GradientBoostingClassifier(n_estimators = 90,learning_rate =0.1 )\n",
    "model3 = AdaBoostClassifier(n_estimators =30 ,learning_rate =1)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('gbm', model2), ('adab', model3)], voting='soft')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble classifier accuracy:  0.9819354939117646\n"
     ]
    }
   ],
   "source": [
    "print (\"Ensemble classifier accuracy: \",np.mean(cross_val_score(model, X, y,cv=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble classifier AUC score:\n",
      "0.8510716543330983\n"
     ]
    }
   ],
   "source": [
    "print(\"Ensemble classifier AUC score:\")\n",
    "print(np.mean(cross_val_score(model, X, y, scoring = 'roc_auc',cv=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     42617\n",
      "         1.0       0.00      0.00      0.00       783\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     43400\n",
      "   macro avg       0.49      0.50      0.50     43400\n",
      "weighted avg       0.96      0.98      0.97     43400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "originalclass = []\n",
    "predictedclass = []\n",
    "nested_score = cross_val_score(model, X, y, cv=10,\\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(classification_report(originalclass, predictedclass)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
